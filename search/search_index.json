{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"aprofs Package - Aproximate Predictions for Feature Selection","text":"<p>This site contains the project documentation for the <code>aprofs</code> project.</p> <p>This is a side project that allows to implement some of the ideas that came from the investigation about how can we use shapley values to get approximate predictions.</p> <p>In here we take account og the additive nature of shapley values, and disregard a bit the potential problems with correlations/interaction that happened.</p> <p>You can install it from github like this: <pre><code>pip install git+https://github.com/blewy/aprofs\n</code></pre></p> <p>or from, pypy production like this:</p> <pre><code>pip install aprofs==0.0.5\n</code></pre> <p>The idea of the package is to help you select your features, getting a simpler model, then understand your features using shapley values and the concept of approximate prediction.</p> <p>Always trying to use the marginal effect of the calculated shapley values.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<p>The documentation consists of four separate parts:</p> <ol> <li>Tutorials Basic Tutorial on how to use the package with some example data.</li> <li>How-To Guides Explain the intended utilization of the package functionality</li> <li>Reference Reference code, math and explanations of the built in functionality</li> <li>API Code documentation</li> </ol> <p>Looking into what helps you, and if needed give us some feedback on git repository.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>I want to thank my colleagues in insurance that trough the years have challenge me to think deeply about these topics.</p>"},{"location":"Tutorial/","title":"Tutorial","text":"In\u00a0[1]: Copied! <pre>from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n</pre> from sklearn.metrics import roc_auc_score from sklearn.model_selection import train_test_split import pandas as pd import warnings  warnings.filterwarnings(\"ignore\", category=DeprecationWarning) <p>Import aprofs package modules code and utils</p> In\u00a0[2]: Copied! <pre>from aprofs import code, models, utils\n</pre> from aprofs import code, models, utils <pre>/Users/filipesantos/Documents/Projects/aprofs/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Get the insurance data from kaggle here : https://www.kaggle.com/datasets/mirichoi0218/insurance</p> In\u00a0[3]: Copied! <pre># Read the CSV file from the data folder\ndata = pd.read_csv(\"insurance.csv\")\n</pre> # Read the CSV file from the data folder data = pd.read_csv(\"insurance.csv\") In\u00a0[4]: Copied! <pre>data\n</pre> data Out[4]: age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 ... ... ... ... ... ... ... ... 1333 50 male 30.970 3 no northwest 10600.54830 1334 18 female 31.920 0 no northeast 2205.98080 1335 18 female 36.850 0 no southeast 1629.83350 1336 21 female 25.800 0 no southwest 2007.94500 1337 61 female 29.070 0 yes northwest 29141.36030 <p>1338 rows \u00d7 7 columns</p> <p>Some simple feature engineering</p> In\u00a0[5]: Copied! <pre># foor loop over a pandas dataframe columns and chnate the typos off all string columns to category\nfor col in data.select_dtypes(include=\"object\").columns:\n    data[col] = data[col].astype(\"category\")\n# Display the data\ndata['is_female'] = (data['sex'] == 'female').astype(int) # is female becomes the target variable\ndata = data.drop(columns=[\"sex\"])\ndata\n</pre> # foor loop over a pandas dataframe columns and chnate the typos off all string columns to category for col in data.select_dtypes(include=\"object\").columns:     data[col] = data[col].astype(\"category\") # Display the data data['is_female'] = (data['sex'] == 'female').astype(int) # is female becomes the target variable data = data.drop(columns=[\"sex\"]) data Out[5]: age bmi children smoker region charges is_female 0 19 27.900 0 yes southwest 16884.92400 1 1 18 33.770 1 no southeast 1725.55230 0 2 28 33.000 3 no southeast 4449.46200 0 3 33 22.705 0 no northwest 21984.47061 0 4 32 28.880 0 no northwest 3866.85520 0 ... ... ... ... ... ... ... ... 1333 50 30.970 3 no northwest 10600.54830 0 1334 18 31.920 0 no northeast 2205.98080 1 1335 18 36.850 0 no southeast 1629.83350 1 1336 21 25.800 0 no southwest 2007.94500 1 1337 61 29.070 0 yes northwest 29141.36030 1 <p>1338 rows \u00d7 7 columns</p> <p>Setting the roles of the features</p> In\u00a0[6]: Copied! <pre>target = \"is_female\"  # target\nT = \"charges\"  # Treatment as price change\nfeatures = [\n    \"age\",\n    \"bmi\",\n    \"children\",\n    \"smoker\",\n    \"region\",\n    \"charges\",\n]\n</pre> target = \"is_female\"  # target T = \"charges\"  # Treatment as price change features = [     \"age\",     \"bmi\",     \"children\",     \"smoker\",     \"region\",     \"charges\", ]  <p>Simple stratified split</p> In\u00a0[7]: Copied! <pre>seed = 42\nX, y = data[features], data[target]\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, random_state=seed)\n</pre>  seed = 42 X, y = data[features], data[target] X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, random_state=seed) <p>Build your initial model, its a tree bases GBM for classification. Note: we want to work with the probabilities not the class prediction</p> In\u00a0[8]: Copied! <pre>from lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\nmonotone_constraints = [1 if col == T else 0 for col in features]\n\ncallbacks = [lgb.early_stopping(10, verbose=0), lgb.log_evaluation(period=0)]\n\n\nmodel = LGBMClassifier(\n    verbose=-1, n_estimators=100, monotone_constraints=monotone_constraints,random_state=seed\n).fit(\n    X_train,\n    y_train,\n    eval_set=[(X_valid, y_valid)],\n    callbacks=callbacks,\n)\npred_valid = model.predict_proba(X_valid)\n</pre> from lightgbm import LGBMClassifier import lightgbm as lgb  monotone_constraints = [1 if col == T else 0 for col in features]  callbacks = [lgb.early_stopping(10, verbose=0), lgb.log_evaluation(period=0)]   model = LGBMClassifier(     verbose=-1, n_estimators=100, monotone_constraints=monotone_constraints,random_state=seed ).fit(     X_train,     y_train,     eval_set=[(X_valid, y_valid)],     callbacks=callbacks, ) pred_valid = model.predict_proba(X_valid) In\u00a0[9]: Copied! <pre>print(f\"The original prediction has an AUC of {roc_auc_score(y_valid, pred_valid[:, 1])}\")\n</pre> print(f\"The original prediction has an AUC of {roc_auc_score(y_valid, pred_valid[:, 1])}\") <pre>The original prediction has an AUC of 0.5312254936907392\n</pre> In\u00a0[10]: Copied! <pre>feature_importance_df = model.feature_importances_\nfeature_names = model.feature_name_\nfeature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": feature_importance_df})\n\n# Sort the DataFrame by importance\nfeature_importance_df.sort_values(\"Importance\", ascending=False, inplace=True)\n\nprint(feature_importance_df)\n</pre> feature_importance_df = model.feature_importances_ feature_names = model.feature_name_ feature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": feature_importance_df})  # Sort the DataFrame by importance feature_importance_df.sort_values(\"Importance\", ascending=False, inplace=True)  print(feature_importance_df) <pre>    Feature  Importance\n1       bmi          56\n0       age          49\n2  children          20\n5   charges          19\n3    smoker           5\n4    region           1\n</pre> <p>Getting your shapley values from the shap package TreeExplainer</p> In\u00a0[11]: Copied! <pre>from shap import TreeExplainer, Explainer\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nshap_explainer = TreeExplainer(model)\nshap_valid = shap_explainer.shap_values(X_valid)\nshap_expected_value = shap_explainer.expected_value\n</pre> from shap import TreeExplainer, Explainer warnings.filterwarnings(\"ignore\", category=UserWarning)  shap_explainer = TreeExplainer(model) shap_valid = shap_explainer.shap_values(X_valid) shap_expected_value = shap_explainer.expected_value <p>Putting you shapley values into a pandas dataframe</p> In\u00a0[12]: Copied! <pre>shaps_values = pd.DataFrame(shap_valid, index=X_valid.index, columns=X_valid.columns)\n</pre> shaps_values = pd.DataFrame(shap_valid, index=X_valid.index, columns=X_valid.columns) In\u00a0[13]: Copied! <pre>shaps_values\n</pre> shaps_values Out[13]: age bmi children smoker region charges 563 -0.025771 -0.000257 0.035974 0.050899 -0.010099 0.036391 1327 -0.058785 -0.087476 -0.002521 0.038555 0.006270 0.024491 1114 0.169415 -0.033496 0.059202 0.027443 -0.003710 -0.068301 678 0.011816 0.015024 0.032997 0.056120 -0.006188 0.070275 490 0.249472 0.100665 0.048708 0.054452 0.001997 -0.370703 ... ... ... ... ... ... ... 1225 -0.084233 -0.104990 0.025577 0.031098 -0.010582 -0.015548 956 -0.057537 -0.084549 0.006919 -0.151000 0.000211 0.067590 189 -0.041293 -0.089155 -0.030936 0.027893 -0.004784 -0.029952 265 0.027805 -0.039058 -0.012704 -0.108263 -0.002953 0.095409 135 0.079413 0.086040 0.051731 0.038937 0.003567 -0.061513 <p>335 rows \u00d7 6 columns</p> <p>Instantiate one of link models to use with with our model. In this case its a Classification problem and we want to use the logistic function.</p> In\u00a0[14]: Copied! <pre>link_model = models.ClassificationLogisticLink()\n</pre> link_model = models.ClassificationLogisticLink() <p>and now create the Aprofs object</p> In\u00a0[15]: Copied! <pre>apos_test = code.Aprofs(\n    X_valid, y_valid, link_model=link_model\n)  # Initialize the Aprofs class with the data and the target columns\n</pre>  apos_test = code.Aprofs(     X_valid, y_valid, link_model=link_model )  # Initialize the Aprofs class with the data and the target columns In\u00a0[16]: Copied! <pre>apos_test\n</pre> apos_test Out[16]: <pre>Aprofs(current_data shape =(335, 6), target_column =[0 1]\n  Shapley values have not been calculated!</pre> <p>See above that the Shapley values have not been added yet to the aprofs object</p> <p>In here we use the built int method to calculate the shapley values and add those into the aprofs object</p> In\u00a0[17]: Copied! <pre>apos_test.calculate_shaps(model, type_model='tree')\n</pre> apos_test.calculate_shaps(model, type_model='tree')  <p>lets take a look inside</p> In\u00a0[18]: Copied! <pre>apos_test\n</pre> apos_test Out[18]: <pre>Aprofs(current_data shape =(335, 6), target_column =[0 1], shap_mean=0.012810903840487525, shap_values.shape=(335, 6)</pre> <p>Now you can see that the shap information was added.</p> <p>Lets take a look at the shap_values table inside the paorfs objects</p> In\u00a0[19]: Copied! <pre>apos_test.shap_values\n</pre> apos_test.shap_values Out[19]: age bmi children smoker region charges 563 -0.025771 -0.000257 0.035974 0.050899 -0.010099 0.036391 1327 -0.058785 -0.087476 -0.002521 0.038555 0.006270 0.024491 1114 0.169415 -0.033496 0.059202 0.027443 -0.003710 -0.068301 678 0.011816 0.015024 0.032997 0.056120 -0.006188 0.070275 490 0.249472 0.100665 0.048708 0.054452 0.001997 -0.370703 ... ... ... ... ... ... ... 1225 -0.084233 -0.104990 0.025577 0.031098 -0.010582 -0.015548 956 -0.057537 -0.084549 0.006919 -0.151000 0.000211 0.067590 189 -0.041293 -0.089155 -0.030936 0.027893 -0.004784 -0.029952 265 0.027805 -0.039058 -0.012704 -0.108263 -0.002953 0.095409 135 0.079413 0.086040 0.051731 0.038937 0.003567 -0.061513 <p>335 rows \u00d7 6 columns</p> In\u00a0[20]: Copied! <pre>apos_test.shap_mean\n</pre> apos_test.shap_mean Out[20]: <pre>0.012810903840487525</pre> <p>Looks the same as the calculated above using the TreeExplainer</p> <p>And the performance is the same using the shapley values and the predictive model, use the .get_feature_performance method to test.</p> In\u00a0[21]: Copied! <pre>perf = apos_test.get_feature_performance(features)\nprint(f\"The performance calculated using the shapley value has an AUC of {perf}\")\n</pre> perf = apos_test.get_feature_performance(features) print(f\"The performance calculated using the shapley value has an AUC of {perf}\") <pre>The performance calculated using the shapley value has an AUC of 0.5315819490981678\n</pre> In\u00a0[22]: Copied! <pre>best_solution = apos_test.brute_force_selection(features)\nbest_solution\n</pre> best_solution = apos_test.brute_force_selection(features) best_solution <pre>Processing 63 combinations:   0%|          | 0/63 [00:00&lt;?, ?it/s]</pre> <pre>Processing 63 combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63/63 [00:00&lt;00:00, 456.01it/s]</pre> <pre>the best list is ('smoker', 'region', 'charges') with performance 0.5942111641833606\n</pre> <pre>\n</pre> Out[22]: <pre>['smoker', 'region', 'charges']</pre> In\u00a0[23]: Copied! <pre>monotone_constraints = [1 if col == T else 0 for col in X_train[best_solution].columns]\n\nmodel_best = LGBMClassifier(\n    verbose=-1, n_estimators=100, monotone_constraints=monotone_constraints, random_state=seed\n).fit(\n    X_train[best_solution],\n    y_train,\n    eval_set=[(X_valid[best_solution], y_valid)],\n    callbacks=callbacks,\n)\npred_valid_best = model_best.predict_proba(X_valid[best_solution])\n</pre> monotone_constraints = [1 if col == T else 0 for col in X_train[best_solution].columns]  model_best = LGBMClassifier(     verbose=-1, n_estimators=100, monotone_constraints=monotone_constraints, random_state=seed ).fit(     X_train[best_solution],     y_train,     eval_set=[(X_valid[best_solution], y_valid)],     callbacks=callbacks, ) pred_valid_best = model_best.predict_proba(X_valid[best_solution]) In\u00a0[24]: Copied! <pre>print(f\"The new prediction have an AUC of {roc_auc_score(y_valid, pred_valid_best[:, 1])}\")\n</pre> print(f\"The new prediction have an AUC of {roc_auc_score(y_valid, pred_valid_best[:, 1])}\") <pre>The new prediction have an AUC of 0.5496542382547943\n</pre> <p>Lets create a new Aprofs object with the new data and model</p> In\u00a0[25]: Copied! <pre>apos_test_best = code.Aprofs(X_valid[best_solution], y_valid, link_model=None)\napos_test_best.calculate_shaps(model_best)\n</pre> apos_test_best = code.Aprofs(X_valid[best_solution], y_valid, link_model=None) apos_test_best.calculate_shaps(model_best) <p>and take a look inside</p> In\u00a0[26]: Copied! <pre>apos_test_best\n</pre> apos_test_best Out[26]: <pre>Aprofs(current_data shape =(335, 3), target_column =[0 1], shap_mean=-0.028112216240915356, shap_values.shape=(335, 3)</pre> <p>We can also try to go a greedy forward selection, it basically look at the approximate prediction for each features, select the best one and adds the on to the list. Then goes and select the next best one and add it to the list. It stops if adding the feature does not make the model better.</p> In\u00a0[27]: Copied! <pre>apos_test.gready_forward_selection(features)\n</pre> apos_test.gready_forward_selection(features) <pre>the best feature to add is smoker with performance 0.5588864333071932\nthe best feature to add is charges with performance 0.589505952805304\nthe best feature to add is region with performance 0.5942111641833606\nThe feature age wont be added\nThe feature children wont be added\nThe feature bmi wont be added\n</pre> Out[27]: <pre>['smoker', 'charges', 'region']</pre> In\u00a0[28]: Copied! <pre>shap_p_values = apos_test.get_shap_p_value(features=features)\nshap_p_values\n</pre> shap_p_values = apos_test.get_shap_p_value(features=features) shap_p_values <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:03&lt;00:00,  1.97it/s]\n</pre> Out[28]: Feature p-value_shap 0 age 0.324 1 bmi 0.800 2 children 0.934 3 smoker 0.086 4 region 0.156 5 charges 0.000 In\u00a0[29]: Copied! <pre>warnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nmerged_df_shap = shap_p_values.merge(feature_importance_df, on=\"Feature\")\nmerged_df_shap.sort_values(\"Importance\", ascending=False, inplace=True)\n\n# Define a function to apply color formatting\ndef color_format(val):\n    if val &lt; 0.05:\n        return \"background-color: green\"\n    elif val &gt; 0.3:\n        return \"background-color: red\"\n    else:\n        return \"background-color: gray\"\n\n\n# Apply color formatting to the dataframe\nstyled_df_shap = merged_df_shap.style.applymap(color_format, subset=[\"p-value_shap\"])\n\n# Display the styled dataframe\nstyled_df_shap\n</pre> warnings.filterwarnings(\"ignore\", category=FutureWarning)  merged_df_shap = shap_p_values.merge(feature_importance_df, on=\"Feature\") merged_df_shap.sort_values(\"Importance\", ascending=False, inplace=True)  # Define a function to apply color formatting def color_format(val):     if val &lt; 0.05:         return \"background-color: green\"     elif val &gt; 0.3:         return \"background-color: red\"     else:         return \"background-color: gray\"   # Apply color formatting to the dataframe styled_df_shap = merged_df_shap.style.applymap(color_format, subset=[\"p-value_shap\"])  # Display the styled dataframe styled_df_shap Out[29]: Feature p-value_shap Importance 1 bmi 0.800000 56 0 age 0.324000 49 2 children 0.934000 20 5 charges 0.000000 19 3 smoker 0.086000 5 4 region 0.156000 1 In\u00a0[30]: Copied! <pre>apos_test.visualize_feature(main_feature=\"children\",other_features=None, nbins=10)\n</pre> apos_test.visualize_feature(main_feature=\"children\",other_features=None, nbins=10) In\u00a0[31]: Copied! <pre>apos_test.visualize_feature(main_feature=\"smoker\")\n</pre> apos_test.visualize_feature(main_feature=\"smoker\") In\u00a0[32]: Copied! <pre>apos_test.visualize_feature(main_feature=\"charges\",other_features=None, nbins=10, type_bins=\"cut\")\n</pre> apos_test.visualize_feature(main_feature=\"charges\",other_features=None, nbins=10, type_bins=\"cut\") <p>And we can even compare the aprofs object comparing the pdp plot for the feature fo the models. If needs to be a feature used on both models... what else to compare!</p> In\u00a0[33]: Copied! <pre>apos_test.compare_feature(apos_test_best,\"charges\")\n</pre> apos_test.compare_feature(apos_test_best,\"charges\") In\u00a0[34]: Copied! <pre>apos_test.visualize_neutralized_feature(main_feature=\"children\", neutralize_features=\"charges\")\n</pre> apos_test.visualize_neutralized_feature(main_feature=\"children\", neutralize_features=\"charges\") <p>And we can see that neutralizing shap make things a bit more flat flor the affect of \"Children\"</p>"},{"location":"Tutorial/#tutorial","title":"Tutorial\u00b6","text":""},{"location":"Tutorial/#importing-initial-packages","title":"Importing initial packages\u00b6","text":""},{"location":"Tutorial/#create-an-aprofs-object","title":"Create an aprofs object\u00b6","text":""},{"location":"Tutorial/#feature-selection","title":"Feature selection.\u00b6","text":"<p>To get a list of the best features using a brute force approach its simple, use the .brute_force_selection.</p> <p>This will test all the combinations of features and return a list with the best features</p>"},{"location":"Tutorial/#calculating-p-values","title":"Calculating p-values\u00b6","text":"<p>We can check what is the quality of our features using a p-value estimate of the change in performance if we shuffle the shapley value. Please look at the references to know more.</p>"},{"location":"Tutorial/#visualization-partial-dependence-plots-to-understand-the-features","title":"Visualization: partial dependence plots to understand the features\u00b6","text":""},{"location":"Tutorial/#comparing-aprofs-objects-shapley-values","title":"Comparing aprofs objects (shapley values)\u00b6","text":""},{"location":"Tutorial/#comparing-the-behavior-of-feature-after-neutralize-the-marginal-effect-anothersame-feature","title":"Comparing the behavior of feature after neutralize the marginal effect another/same feature\u00b6","text":"<p>Basically here the trick is to just average our the shapley values of the features that we want to neutralize and use that to calculate the final prediction.</p> <p>Example: we have feature A, feature B and feature C. What will will do to neutralize Feature B of fo use for prediction the following:</p> <pre><code>    - Shappley Feature A + Overall Shappley Average Feature B + Shappley Featue C</code></pre>"},{"location":"api/","title":"References","text":"<p>Core code development of the Aprofs class.</p> <p>This class is used to calculate the SHAP values of a model and evaluate the performance of the features based on the SHAP values. The class also provides a method to visualize the marginal effect of a feature on the target variable. The class is used to perform feature selection based on the SHAP values and to calculate the p-values of the SHAP values of the features. The class is also used to calculate the performance of the model based on the SHAP values of the features.</p>"},{"location":"api/#src.aprofs.code.Aprofs","title":"<code>Aprofs</code>","text":"<p>Aprofs Class</p>"},{"location":"api/#src.aprofs.code.Aprofs--a-class-for-analyzing-shap-values-using-approximate-predictions","title":"A class for analyzing SHAP values using approximate predictions.","text":"<p>Attributes:</p> Name Type Description <code>current_data</code> <code>DataFrame</code> <p>The current data.</p> <code>target_column</code> <code>Series</code> <p>The target column.</p> <code>link</code> <code>function</code> <p>The link function.</p> <code>link_srt</code> <code>str</code> <p>The string representation of the link function.</p> <code>shap_mean</code> <code>float</code> <p>The mean SHAP value. None if SHAP values have not been calculated.</p> <code>shap_values</code> <code>DataFrame</code> <p>The SHAP values. None if SHAP values have not been calculated.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>class Aprofs:\n    \"\"\"\n    Aprofs Class\n\n    A class for analyzing SHAP values using approximate predictions.\n    --------------------------------------------------------------\n\n\n    Attributes:\n        current_data (pd.DataFrame): The current data.\n        target_column (Series): The target column.\n        link (function): The link function.\n        link_srt (str): The string representation of the link function.\n        shap_mean (float): The mean SHAP value. None if SHAP values have not been calculated.\n        shap_values (DataFrame): The SHAP values. None if SHAP values have not been calculated.\n\n    \"\"\"\n\n    def __init__(self, current_data, target_column, link_model: LinkModels):\n        self.current_data = current_data\n        self.target_column = target_column\n        self.link_model = ClassificationLogisticLink() if link_model is None else link_model\n        self.shap_mean: float = None\n        self.shap_values: pd.DataFrame = None\n\n    def __repr__(self):\n        return (\n            f\"Aprofs(current_data shape ={self.current_data.shape}, target_column ={self.target_column.unique()}\"\n            + (\n                f\", shap_mean={self.shap_mean}, shap_values.shape={self.shap_values.shape}\"\n                if self.shap_mean is not None\n                else \"\\n  Shapley values have not been calculated!\"\n            )\n        )\n\n    def calculate_shaps(self, model: Any, type_model=\"tree\") -&gt; None:\n        \"\"\"\n        Calculate the SHAP values for the given model.\n\n        Parameters:\n            model (Any): The trained model for which to calculate the SHAP values.\n            type_model (str): type of model: tree based or other. If \"tree\" then TreeExplainer will be use, otherwise a general explainer from the SHAP package is used. Defaults to 'tree'.\n\n\n        Returns:\n            None\n        \"\"\"\n        shap_values, shap_mean = utils.get_shap_values(self.current_data, model, type_model)\n        self.shap_values = pd.DataFrame(shap_values, index=self.current_data.index, columns=self.current_data.columns)\n        self.shap_mean = shap_mean\n\n    def get_feature_performance(self, features: List[str]) -&gt; float:\n        \"\"\"\n        Calculate the performance of the features based on the SHAP values.\n\n        Parameters:\n            features (List[str]): The list of features for which to calculate the performance.\n\n        Returns:\n            float: The performance of the features based on the SHAP values.\n\n        Raises:\n            ValueError: If an any feature is missing in the SHAP values.\n        \"\"\"\n        missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n        if missing_features:\n            raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n        return self.link_model.performance_fit(\n            self.target_column, utils.calculate_row_sum(self.shap_values, self.shap_mean, features, self.link_model)\n        )\n\n    def brute_force_selection(self, features: List[str]) -&gt; List[str]:\n        \"\"\"\n        Perform brute force feature selection by evaluating the performance of all possible combinations of features.\n\n        Parameters:\n            features (List[str]): The list of features to consider for feature selection.\n\n        Returns:\n            List[str]: The best list of features with the highest performance.\n\n        Raises:\n            ValueError: If an any feature is missing in the SHAP values.\n        \"\"\"\n        missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n        if missing_features:\n            raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n        best_performance = 0.0\n        best_list = []\n        all_combinations = list(utils.generate_all_combinations(features))\n        for comb in tqdm(all_combinations, desc=f\"Processing {len(all_combinations)} combinations\"):\n            current_performance = self.get_feature_performance(list(comb))\n            if current_performance &gt; best_performance:\n                best_performance = current_performance\n                best_list = comb\n        print(f\"the best list is {best_list} with performance {best_performance}\")\n        return list(best_list)\n\n    def gready_forward_selection(self, features: List[str], greediness: float = 0.001) -&gt; List[str]:\n        \"\"\"\n        Perform gready forward feature selection by evaluating the performance of all possible combinations of features.\n\n        Parameters:\n            features (List[str]): The list of features to consider for feature selection.\n            greediness (float): The greediness factor, how much better needs to be the performance to add the feature. Default is 0.001.\n        Returns:\n            List[str]: The best list of features with the highest performance.\n\n        Raises:\n            ValueError: If an any feature is missing in the SHAP values.\n        \"\"\"\n        missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n        if missing_features:\n            raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n        best_list: List = []\n        candidate_list: List[str] = features.copy()\n        aproximate_performance: List[float] = []\n        best_performance = 0.0\n        while len(candidate_list) &gt; 0:\n            best_feature_, best_performance_ = utils.best_feature(\n                self.shap_values, self.shap_mean, self.link_model, self.target_column, best_list, candidate_list\n            )\n            candidate_list.remove(best_feature_)\n\n            if self.link_model.perform == \"maximize\":  # maximize metric\n                if best_performance &gt; best_performance_ * (1 + greediness):\n                    print(f\"The feature {best_feature_} wont be added\")\n                else:\n                    best_performance = best_performance_\n                    best_list.append(best_feature_)\n                    print(f\"the best feature to add is {best_feature_} with performance {best_performance_}\")\n                    aproximate_performance.append(best_performance_)\n\n            if self.link_model.perform == \"minimize\":  # minimize metric\n                if best_performance &lt; best_performance_ * (1 - greediness):\n                    print(f\"The feature {best_feature_} wont be added\")\n                else:\n                    best_performance = best_performance_\n                    best_list.append(best_feature_)\n                    print(f\"the best feature to add is {best_feature_} with performance {best_performance_}\")\n                    aproximate_performance.append(best_performance_)\n\n        return best_list\n\n    def get_shap_p_value(self, features: List[str], suffle_size: int = 500) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the p-values of the SHAP values of the features.\n\n        Parameters:\n            features (List[str]): The list of features for which to calculate the p-values.\n            suffle_size (int): The number of shuffling iterations to perform. Default is 500.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the features and their corresponding p-values.\n\n        Raises:\n            ValueError: If an any feature is missing in the SHAP values.\n        \"\"\"\n        missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n        if missing_features:\n            raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n        p_values = []\n        performance_threshold = self.get_feature_performance(self.shap_values.columns)\n        for feature in tqdm(features):\n            samples = [\n                utils.random_sort_shaps(self.shap_values, self.shap_mean, feature, self.target_column, self.link_model)\n                for _ in range(suffle_size)\n            ]\n            count = sum(sample &gt; performance_threshold for sample in samples)\n            p_values.append(count / suffle_size)\n\n        return pd.DataFrame({\"Feature\": features, \"p-value_shap\": p_values})\n\n    def visualize_feature(  # pylint: disable=too-many-arguments\n        self,\n        main_feature: str,\n        other_features: List[str] = None,\n        nbins: int = 20,\n        type_bins: str = \"qcut\",\n        type_plot: str = \"prob\",\n    ) -&gt; None:\n        \"\"\"\n        Visualize the marginal effect of a feature on the target variable.\n\n        Parameters:\n            main_feature (str): The main feature for which to visualize the marginal effect.\n            other_features (List[str]): The list of other features to include in the visualization. Default is None.\n            nbins (int): The number of bins to use for the visualization. Default is 20.\n            type_bins (str): The type of binning to use. Default is \"qcut\".\n            type_plot (str): The type of plot to generate. Default is \"prob\".\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If an any feature is missing in the SHAP values dataframe.\n        \"\"\"\n        # generate data to plot marginal effect shapley values\n        if other_features is None:\n            other_features = []\n        features = []\n        features.append(main_feature)\n        features.extend(other_features)\n\n        missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n        if missing_features:\n            raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n        temp_data = utils.temp_plot_data(self, features)\n        # call plotting function\n        utils.plot_data(\n            temp_data,\n            main_feature,\n            other_features=other_features,\n            nbins=nbins,\n            type_bins=type_bins,\n            type_plot=type_plot,\n        )\n\n    def compare_feature(  # pylint: disable=too-many-arguments\n        self,\n        other,\n        feature: str,\n        nbins: int = 20,\n        type_bins: str = \"qcut\",\n        type_plot: str = \"prob\",\n    ) -&gt; None:\n        \"\"\"\n        Visualize the marginal effect of a feature on the target variable.\n\n        Parameters:\n            feature (str): The main feature for which to visualize the marginal effect.\n            nbins (int): The number of bins to use for the visualization. Default is 20.\n            type_bins (str): The type of binning to use. Default is \"qcut\".\n            type_plot (str): The type of plot to generate. Default is \"prob\".\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If an any feature is missing in the SHAP values.\n        \"\"\"\n\n        if not isinstance(other, Aprofs):\n            raise ValueError(\"Can only compare with another Aprofs object\")\n\n        if feature not in self.shap_values.columns:\n            raise ValueError(f\"The following feature are missing in the SHAP values: {feature}\")\n\n        temp_data = utils.temp_plot_compare_data(self, other, feature)\n        # call plotting function\n        utils.plot_data_compare(\n            temp_data,\n            feature,\n            nbins=nbins,\n            type_bins=type_bins,\n            type_plot=type_plot,\n        )\n\n    def visualize_neutralized_feature(  # pylint: disable=too-many-arguments\n        self,\n        main_feature: str,\n        neutralize_features: List[str] = None,\n        nbins: int = 20,\n        type_bins: str = \"qcut\",\n        type_plot: str = \"prob\",\n    ) -&gt; None:\n        \"\"\"\n        Visualize the marginal effect of a feature on the target variable after neutralizing the effect of other features.\n\n        Parameters:\n            main_feature (str): The main feature for which to visualize the marginal effect.\n            neutralize_features (List[str]): The list of other features to be neutralized.\n            nbins (int): The number of bins to use for the visualization. Default is 20.\n            type_bins (str): The type of binning to use. Default is \"qcut\".\n            type_plot (str): The type of plot to generate. Default is \"prob\".\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If an any feature is missing in the SHAP values dataframe.\n        \"\"\"\n        # generate data to plot marginal effect shapley values\n        if neutralize_features is None:\n            neutralize_features = []\n        features = []\n        if not isinstance(neutralize_features, list):\n            neutralize_features = [neutralize_features]\n\n        features.append(main_feature)\n        features.extend(neutralize_features)\n        features = list(set(features))  # remove duplicates\n\n        missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n        if missing_features:\n            raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n        temp_data = utils.temp_neutral_plot_data(self, neutralize_features)\n        temp_data[main_feature] = self.current_data[main_feature]\n        # call plotting function\n        utils.plot_data_neutral(\n            temp_data,\n            main_feature,\n            nbins=nbins,\n            type_bins=type_bins,\n            type_plot=type_plot,\n        )\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.brute_force_selection","title":"<code>brute_force_selection(features)</code>","text":"<p>Perform brute force feature selection by evaluating the performance of all possible combinations of features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>The list of features to consider for feature selection.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The best list of features with the highest performance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an any feature is missing in the SHAP values.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def brute_force_selection(self, features: List[str]) -&gt; List[str]:\n    \"\"\"\n    Perform brute force feature selection by evaluating the performance of all possible combinations of features.\n\n    Parameters:\n        features (List[str]): The list of features to consider for feature selection.\n\n    Returns:\n        List[str]: The best list of features with the highest performance.\n\n    Raises:\n        ValueError: If an any feature is missing in the SHAP values.\n    \"\"\"\n    missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n    if missing_features:\n        raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n    best_performance = 0.0\n    best_list = []\n    all_combinations = list(utils.generate_all_combinations(features))\n    for comb in tqdm(all_combinations, desc=f\"Processing {len(all_combinations)} combinations\"):\n        current_performance = self.get_feature_performance(list(comb))\n        if current_performance &gt; best_performance:\n            best_performance = current_performance\n            best_list = comb\n    print(f\"the best list is {best_list} with performance {best_performance}\")\n    return list(best_list)\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.calculate_shaps","title":"<code>calculate_shaps(model, type_model='tree')</code>","text":"<p>Calculate the SHAP values for the given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The trained model for which to calculate the SHAP values.</p> required <code>type_model</code> <code>str</code> <p>type of model: tree based or other. If \"tree\" then TreeExplainer will be use, otherwise a general explainer from the SHAP package is used. Defaults to 'tree'.</p> <code>'tree'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def calculate_shaps(self, model: Any, type_model=\"tree\") -&gt; None:\n    \"\"\"\n    Calculate the SHAP values for the given model.\n\n    Parameters:\n        model (Any): The trained model for which to calculate the SHAP values.\n        type_model (str): type of model: tree based or other. If \"tree\" then TreeExplainer will be use, otherwise a general explainer from the SHAP package is used. Defaults to 'tree'.\n\n\n    Returns:\n        None\n    \"\"\"\n    shap_values, shap_mean = utils.get_shap_values(self.current_data, model, type_model)\n    self.shap_values = pd.DataFrame(shap_values, index=self.current_data.index, columns=self.current_data.columns)\n    self.shap_mean = shap_mean\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.compare_feature","title":"<code>compare_feature(other, feature, nbins=20, type_bins='qcut', type_plot='prob')</code>","text":"<p>Visualize the marginal effect of a feature on the target variable.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>str</code> <p>The main feature for which to visualize the marginal effect.</p> required <code>nbins</code> <code>int</code> <p>The number of bins to use for the visualization. Default is 20.</p> <code>20</code> <code>type_bins</code> <code>str</code> <p>The type of binning to use. Default is \"qcut\".</p> <code>'qcut'</code> <code>type_plot</code> <code>str</code> <p>The type of plot to generate. Default is \"prob\".</p> <code>'prob'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an any feature is missing in the SHAP values.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def compare_feature(  # pylint: disable=too-many-arguments\n    self,\n    other,\n    feature: str,\n    nbins: int = 20,\n    type_bins: str = \"qcut\",\n    type_plot: str = \"prob\",\n) -&gt; None:\n    \"\"\"\n    Visualize the marginal effect of a feature on the target variable.\n\n    Parameters:\n        feature (str): The main feature for which to visualize the marginal effect.\n        nbins (int): The number of bins to use for the visualization. Default is 20.\n        type_bins (str): The type of binning to use. Default is \"qcut\".\n        type_plot (str): The type of plot to generate. Default is \"prob\".\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If an any feature is missing in the SHAP values.\n    \"\"\"\n\n    if not isinstance(other, Aprofs):\n        raise ValueError(\"Can only compare with another Aprofs object\")\n\n    if feature not in self.shap_values.columns:\n        raise ValueError(f\"The following feature are missing in the SHAP values: {feature}\")\n\n    temp_data = utils.temp_plot_compare_data(self, other, feature)\n    # call plotting function\n    utils.plot_data_compare(\n        temp_data,\n        feature,\n        nbins=nbins,\n        type_bins=type_bins,\n        type_plot=type_plot,\n    )\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.get_feature_performance","title":"<code>get_feature_performance(features)</code>","text":"<p>Calculate the performance of the features based on the SHAP values.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>The list of features for which to calculate the performance.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The performance of the features based on the SHAP values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an any feature is missing in the SHAP values.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def get_feature_performance(self, features: List[str]) -&gt; float:\n    \"\"\"\n    Calculate the performance of the features based on the SHAP values.\n\n    Parameters:\n        features (List[str]): The list of features for which to calculate the performance.\n\n    Returns:\n        float: The performance of the features based on the SHAP values.\n\n    Raises:\n        ValueError: If an any feature is missing in the SHAP values.\n    \"\"\"\n    missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n    if missing_features:\n        raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n    return self.link_model.performance_fit(\n        self.target_column, utils.calculate_row_sum(self.shap_values, self.shap_mean, features, self.link_model)\n    )\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.get_shap_p_value","title":"<code>get_shap_p_value(features, suffle_size=500)</code>","text":"<p>Calculate the p-values of the SHAP values of the features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>The list of features for which to calculate the p-values.</p> required <code>suffle_size</code> <code>int</code> <p>The number of shuffling iterations to perform. Default is 500.</p> <code>500</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the features and their corresponding p-values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an any feature is missing in the SHAP values.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def get_shap_p_value(self, features: List[str], suffle_size: int = 500) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the p-values of the SHAP values of the features.\n\n    Parameters:\n        features (List[str]): The list of features for which to calculate the p-values.\n        suffle_size (int): The number of shuffling iterations to perform. Default is 500.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the features and their corresponding p-values.\n\n    Raises:\n        ValueError: If an any feature is missing in the SHAP values.\n    \"\"\"\n    missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n    if missing_features:\n        raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n    p_values = []\n    performance_threshold = self.get_feature_performance(self.shap_values.columns)\n    for feature in tqdm(features):\n        samples = [\n            utils.random_sort_shaps(self.shap_values, self.shap_mean, feature, self.target_column, self.link_model)\n            for _ in range(suffle_size)\n        ]\n        count = sum(sample &gt; performance_threshold for sample in samples)\n        p_values.append(count / suffle_size)\n\n    return pd.DataFrame({\"Feature\": features, \"p-value_shap\": p_values})\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.gready_forward_selection","title":"<code>gready_forward_selection(features, greediness=0.001)</code>","text":"<p>Perform gready forward feature selection by evaluating the performance of all possible combinations of features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>The list of features to consider for feature selection.</p> required <code>greediness</code> <code>float</code> <p>The greediness factor, how much better needs to be the performance to add the feature. Default is 0.001.</p> <code>0.001</code> <p>Returns:     List[str]: The best list of features with the highest performance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an any feature is missing in the SHAP values.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def gready_forward_selection(self, features: List[str], greediness: float = 0.001) -&gt; List[str]:\n    \"\"\"\n    Perform gready forward feature selection by evaluating the performance of all possible combinations of features.\n\n    Parameters:\n        features (List[str]): The list of features to consider for feature selection.\n        greediness (float): The greediness factor, how much better needs to be the performance to add the feature. Default is 0.001.\n    Returns:\n        List[str]: The best list of features with the highest performance.\n\n    Raises:\n        ValueError: If an any feature is missing in the SHAP values.\n    \"\"\"\n    missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n    if missing_features:\n        raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n    best_list: List = []\n    candidate_list: List[str] = features.copy()\n    aproximate_performance: List[float] = []\n    best_performance = 0.0\n    while len(candidate_list) &gt; 0:\n        best_feature_, best_performance_ = utils.best_feature(\n            self.shap_values, self.shap_mean, self.link_model, self.target_column, best_list, candidate_list\n        )\n        candidate_list.remove(best_feature_)\n\n        if self.link_model.perform == \"maximize\":  # maximize metric\n            if best_performance &gt; best_performance_ * (1 + greediness):\n                print(f\"The feature {best_feature_} wont be added\")\n            else:\n                best_performance = best_performance_\n                best_list.append(best_feature_)\n                print(f\"the best feature to add is {best_feature_} with performance {best_performance_}\")\n                aproximate_performance.append(best_performance_)\n\n        if self.link_model.perform == \"minimize\":  # minimize metric\n            if best_performance &lt; best_performance_ * (1 - greediness):\n                print(f\"The feature {best_feature_} wont be added\")\n            else:\n                best_performance = best_performance_\n                best_list.append(best_feature_)\n                print(f\"the best feature to add is {best_feature_} with performance {best_performance_}\")\n                aproximate_performance.append(best_performance_)\n\n    return best_list\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.visualize_feature","title":"<code>visualize_feature(main_feature, other_features=None, nbins=20, type_bins='qcut', type_plot='prob')</code>","text":"<p>Visualize the marginal effect of a feature on the target variable.</p> <p>Parameters:</p> Name Type Description Default <code>main_feature</code> <code>str</code> <p>The main feature for which to visualize the marginal effect.</p> required <code>other_features</code> <code>List[str]</code> <p>The list of other features to include in the visualization. Default is None.</p> <code>None</code> <code>nbins</code> <code>int</code> <p>The number of bins to use for the visualization. Default is 20.</p> <code>20</code> <code>type_bins</code> <code>str</code> <p>The type of binning to use. Default is \"qcut\".</p> <code>'qcut'</code> <code>type_plot</code> <code>str</code> <p>The type of plot to generate. Default is \"prob\".</p> <code>'prob'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an any feature is missing in the SHAP values dataframe.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def visualize_feature(  # pylint: disable=too-many-arguments\n    self,\n    main_feature: str,\n    other_features: List[str] = None,\n    nbins: int = 20,\n    type_bins: str = \"qcut\",\n    type_plot: str = \"prob\",\n) -&gt; None:\n    \"\"\"\n    Visualize the marginal effect of a feature on the target variable.\n\n    Parameters:\n        main_feature (str): The main feature for which to visualize the marginal effect.\n        other_features (List[str]): The list of other features to include in the visualization. Default is None.\n        nbins (int): The number of bins to use for the visualization. Default is 20.\n        type_bins (str): The type of binning to use. Default is \"qcut\".\n        type_plot (str): The type of plot to generate. Default is \"prob\".\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If an any feature is missing in the SHAP values dataframe.\n    \"\"\"\n    # generate data to plot marginal effect shapley values\n    if other_features is None:\n        other_features = []\n    features = []\n    features.append(main_feature)\n    features.extend(other_features)\n\n    missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n    if missing_features:\n        raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n    temp_data = utils.temp_plot_data(self, features)\n    # call plotting function\n    utils.plot_data(\n        temp_data,\n        main_feature,\n        other_features=other_features,\n        nbins=nbins,\n        type_bins=type_bins,\n        type_plot=type_plot,\n    )\n</code></pre>"},{"location":"api/#src.aprofs.code.Aprofs.visualize_neutralized_feature","title":"<code>visualize_neutralized_feature(main_feature, neutralize_features=None, nbins=20, type_bins='qcut', type_plot='prob')</code>","text":"<p>Visualize the marginal effect of a feature on the target variable after neutralizing the effect of other features.</p> <p>Parameters:</p> Name Type Description Default <code>main_feature</code> <code>str</code> <p>The main feature for which to visualize the marginal effect.</p> required <code>neutralize_features</code> <code>List[str]</code> <p>The list of other features to be neutralized.</p> <code>None</code> <code>nbins</code> <code>int</code> <p>The number of bins to use for the visualization. Default is 20.</p> <code>20</code> <code>type_bins</code> <code>str</code> <p>The type of binning to use. Default is \"qcut\".</p> <code>'qcut'</code> <code>type_plot</code> <code>str</code> <p>The type of plot to generate. Default is \"prob\".</p> <code>'prob'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an any feature is missing in the SHAP values dataframe.</p> Source code in <code>src/aprofs/code.py</code> <pre><code>def visualize_neutralized_feature(  # pylint: disable=too-many-arguments\n    self,\n    main_feature: str,\n    neutralize_features: List[str] = None,\n    nbins: int = 20,\n    type_bins: str = \"qcut\",\n    type_plot: str = \"prob\",\n) -&gt; None:\n    \"\"\"\n    Visualize the marginal effect of a feature on the target variable after neutralizing the effect of other features.\n\n    Parameters:\n        main_feature (str): The main feature for which to visualize the marginal effect.\n        neutralize_features (List[str]): The list of other features to be neutralized.\n        nbins (int): The number of bins to use for the visualization. Default is 20.\n        type_bins (str): The type of binning to use. Default is \"qcut\".\n        type_plot (str): The type of plot to generate. Default is \"prob\".\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If an any feature is missing in the SHAP values dataframe.\n    \"\"\"\n    # generate data to plot marginal effect shapley values\n    if neutralize_features is None:\n        neutralize_features = []\n    features = []\n    if not isinstance(neutralize_features, list):\n        neutralize_features = [neutralize_features]\n\n    features.append(main_feature)\n    features.extend(neutralize_features)\n    features = list(set(features))  # remove duplicates\n\n    missing_features = [feature for feature in features if feature not in self.shap_values.columns]\n    if missing_features:\n        raise ValueError(f\"The following features are missing in the SHAP values: {missing_features}\")\n\n    temp_data = utils.temp_neutral_plot_data(self, neutralize_features)\n    temp_data[main_feature] = self.current_data[main_feature]\n    # call plotting function\n    utils.plot_data_neutral(\n        temp_data,\n        main_feature,\n        nbins=nbins,\n        type_bins=type_bins,\n        type_plot=type_plot,\n    )\n</code></pre>"},{"location":"api/#detailed-api-models","title":"Detailed API models","text":"<p>This module implements the models class. this wasy we can extend this class to implement new models to calculate the use with the aprofs class</p>"},{"location":"api/#src.aprofs.models.ClassificationLogisticLink","title":"<code>ClassificationLogisticLink</code>","text":"<p>             Bases: <code>LinkModels</code></p> <p>This class implements the interface for classification with logistic link</p> Source code in <code>src/aprofs/models.py</code> <pre><code>class ClassificationLogisticLink(LinkModels):\n    \"\"\"This class implements the interface for classification with logistic link\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(type_model=\"classification\", type_link=\"logistic\", perform=\"maximize\")\n\n    def performance_fit(self, target: Union[np.ndarray, pd.Series], prediction: Union[np.ndarray, pd.Series]) -&gt; float:\n        return roc_auc_score(target, prediction)\n\n    def link_calculate(\n        self, inv_prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        if not isinstance(inv_prediction, (int, float, np.ndarray, pd.Series)):\n            raise ValueError(\"Invalid input type for link_calculate\")\n        return 1 / (1 + np.exp(-inv_prediction))\n\n    def inv_link_calculate(\n        self, prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        return np.log(prediction / (1 - prediction))\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}() with type model {self.type_model} and type link {self.type_link}\"\n</code></pre>"},{"location":"api/#src.aprofs.models.LinkModels","title":"<code>LinkModels</code>","text":"<p>This class implements the interface for the link models to be used in the aprofs class</p> Functionality that needs ot be implemented <ul> <li>performance_fit: calculate the performance of the model</li> <li>link_calculate: calculate the link function</li> <li>inv_link_calculate: calculate the inverse link function</li> </ul> Source code in <code>src/aprofs/models.py</code> <pre><code>class LinkModels(metaclass=abc.ABCMeta):\n    \"\"\"This class implements the interface for the link models\n    to be used in the aprofs class\n\n    Functionality that needs ot be implemented:\n        - performance_fit: calculate the performance of the model\n        - link_calculate: calculate the link function\n        - inv_link_calculate: calculate the inverse link function\n\n    \"\"\"\n\n    def __init__(self, type_model: str, type_link: str, perform: str) -&gt; None:\n        self.type_model = type_model\n        self.type_link = type_link\n        self.perform = perform\n\n    @abc.abstractmethod\n    def performance_fit(self, target: Union[np.ndarray, pd.Series], prediction: Union[np.ndarray, pd.Series]) -&gt; float:\n        \"\"\"\n        Calculate the performance of the model.\n\n        Args:\n            target (np.ndarray): The true target values.\n            prediction (np.ndarray): The predicted values.\n\n        Returns:\n            float: The performance metric.\n        \"\"\"\n\n    @abc.abstractmethod\n    def link_calculate(\n        self, inv_prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray]:\n        \"\"\"\n        Calculate the link function.\n\n        Args:\n            inv_prediction (Union[int, float, np.ndarray]): The input value(s).\n\n        Returns:\n            Union[int, float, np.ndarray]: The transformed value(s).\n        \"\"\"\n\n    @abc.abstractmethod\n    def inv_link_calculate(\n        self, prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        \"\"\"\n        Calculate the inverse link function.\n\n        Args:\n            prediction (Union[int, float, np.ndarray]): The input value(s).\n\n        Returns:\n            Union[int, float, np.ndarray]: The transformed value(s).\n        \"\"\"\n</code></pre>"},{"location":"api/#src.aprofs.models.LinkModels.inv_link_calculate","title":"<code>inv_link_calculate(prediction)</code>  <code>abstractmethod</code>","text":"<p>Calculate the inverse link function.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Union[int, float, ndarray]</code> <p>The input value(s).</p> required <p>Returns:</p> Type Description <code>Union[int, float, ndarray, Series]</code> <p>Union[int, float, np.ndarray]: The transformed value(s).</p> Source code in <code>src/aprofs/models.py</code> <pre><code>@abc.abstractmethod\ndef inv_link_calculate(\n    self, prediction: Union[int, float, np.ndarray, pd.Series]\n) -&gt; Union[int, float, np.ndarray, pd.Series]:\n    \"\"\"\n    Calculate the inverse link function.\n\n    Args:\n        prediction (Union[int, float, np.ndarray]): The input value(s).\n\n    Returns:\n        Union[int, float, np.ndarray]: The transformed value(s).\n    \"\"\"\n</code></pre>"},{"location":"api/#src.aprofs.models.LinkModels.link_calculate","title":"<code>link_calculate(inv_prediction)</code>  <code>abstractmethod</code>","text":"<p>Calculate the link function.</p> <p>Parameters:</p> Name Type Description Default <code>inv_prediction</code> <code>Union[int, float, ndarray]</code> <p>The input value(s).</p> required <p>Returns:</p> Type Description <code>Union[int, float, ndarray]</code> <p>Union[int, float, np.ndarray]: The transformed value(s).</p> Source code in <code>src/aprofs/models.py</code> <pre><code>@abc.abstractmethod\ndef link_calculate(\n    self, inv_prediction: Union[int, float, np.ndarray, pd.Series]\n) -&gt; Union[int, float, np.ndarray]:\n    \"\"\"\n    Calculate the link function.\n\n    Args:\n        inv_prediction (Union[int, float, np.ndarray]): The input value(s).\n\n    Returns:\n        Union[int, float, np.ndarray]: The transformed value(s).\n    \"\"\"\n</code></pre>"},{"location":"api/#src.aprofs.models.LinkModels.performance_fit","title":"<code>performance_fit(target, prediction)</code>  <code>abstractmethod</code>","text":"<p>Calculate the performance of the model.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>The true target values.</p> required <code>prediction</code> <code>ndarray</code> <p>The predicted values.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The performance metric.</p> Source code in <code>src/aprofs/models.py</code> <pre><code>@abc.abstractmethod\ndef performance_fit(self, target: Union[np.ndarray, pd.Series], prediction: Union[np.ndarray, pd.Series]) -&gt; float:\n    \"\"\"\n    Calculate the performance of the model.\n\n    Args:\n        target (np.ndarray): The true target values.\n        prediction (np.ndarray): The predicted values.\n\n    Returns:\n        float: The performance metric.\n    \"\"\"\n</code></pre>"},{"location":"api/#src.aprofs.models.RegressionIdentityLink","title":"<code>RegressionIdentityLink</code>","text":"<p>             Bases: <code>LinkModels</code></p> <p>This class implements the interface for regression with identity link</p> Source code in <code>src/aprofs/models.py</code> <pre><code>class RegressionIdentityLink(LinkModels):\n    \"\"\"This class implements the interface for regression with identity link\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(type_model=\"regression\", type_link=\"identity\", perform=\"minimize\")\n\n    def performance_fit(self, target: Union[np.ndarray, pd.Series], prediction: Union[np.ndarray, pd.Series]) -&gt; float:\n        return np.sqrt(mean_squared_error(target, prediction))\n\n    def link_calculate(\n        self, inv_prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        return inv_prediction\n\n    def inv_link_calculate(\n        self, prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        return prediction\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}() with type model {self.type_model} and type link {self.type_link}\"\n</code></pre>"},{"location":"api/#src.aprofs.models.RegressionLogLink","title":"<code>RegressionLogLink</code>","text":"<p>             Bases: <code>LinkModels</code></p> <p>This class implements the interface for regression with logarithmic link</p> Source code in <code>src/aprofs/models.py</code> <pre><code>class RegressionLogLink(LinkModels):\n    \"\"\"This class implements the interface for regression with logarithmic link\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(type_model=\"regression\", type_link=\"logarithmic\", perform=\"minimize\")\n\n    def performance_fit(self, target: Union[np.ndarray, pd.Series], prediction: Union[np.ndarray, pd.Series]) -&gt; float:\n        return np.sqrt(mean_squared_error(target, prediction))\n\n    def link_calculate(\n        self, inv_prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        if not isinstance(inv_prediction, (int, float, np.ndarray, pd.Series)):\n            raise ValueError(\"Invalid input type for link_calculate\")\n        return np.log(inv_prediction)\n\n    def inv_link_calculate(\n        self, prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        return np.exp(prediction)\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}() with type model {self.type_model} and type link {self.type_link}\"\n</code></pre>"},{"location":"api/#detailed-api-utilities","title":"Detailed API utilities","text":"<p>Utility functions for the package. this module contains utility functions that are used in the package. the core functions are used to calculate the SHAP values and expected average SHAP value for a given dataset and model.</p>"},{"location":"api/#src.aprofs.utils.best_feature","title":"<code>best_feature(shaps_values, shap_expected_values, link_model, y_target, current_list, candidate_list)</code>","text":"<p>Return the best feature to add to the current list based on the highest AUC score.</p> <p>Parameters:</p> Name Type Description Default <code>shaps_values</code> <code>DataFrame</code> <p>A DataFrame containing SHAP values for each feature.</p> required <code>shap_expected_values</code> <code>Series</code> <p>A Series containing the expected SHAP values.</p> required <code>link_model</code> <code>aprofs model object</code> <p>An object that allows to calculate the performance of the model.</p> required <code>y_target</code> <code>Series</code> <p>The target variable for the AUC score calculation.</p> required <code>current_list</code> <code>list</code> <p>The current list of features.</p> required <code>candidate_list</code> <code>list</code> <p>The list of candidate features to consider adding.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, float]</code> <p>A tuple containing the best feature to add (str) and the corresponding best AUC score (float).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>candidate_list</code> is empty.</p> Source code in <code>src/aprofs/utils.py</code> <pre><code>def best_feature(  # pylint: disable=too-many-arguments\n    shaps_values: pd.DataFrame,\n    shap_expected_values: float,\n    link_model: LinkModels,\n    y_target: pd.Series,\n    current_list: List[str],\n    candidate_list: List[str],\n) -&gt; Tuple[str, float]:\n    \"\"\"\n    Return the best feature to add to the current list based on the highest AUC score.\n\n    Args:\n        shaps_values (DataFrame): A DataFrame containing SHAP values for each feature.\n        shap_expected_values (Series): A Series containing the expected SHAP values.\n        link_model (aprofs model object): An object that allows to calculate the performance of the model.\n        y_target (Series): The target variable for the AUC score calculation.\n        current_list (list): The current list of features.\n        candidate_list (list): The list of candidate features to consider adding.\n\n    Returns:\n        tuple: A tuple containing the best feature to add (str) and the corresponding best AUC score (float).\n\n    Raises:\n        ValueError: If `candidate_list` is empty.\n    \"\"\"\n\n    if candidate_list == [] or candidate_list is None:\n        raise ValueError(\"The candidate list cannot be empty.\")\n\n    best_feature: str = None\n    best_auc: float = 0\n    for feature in candidate_list:\n        current_list.append(feature)\n        aprox_preds = calculate_row_sum(shaps_values, shap_expected_values, current_list, link_model)\n        auc = roc_auc_score(y_target, aprox_preds)\n        if auc &gt; best_auc:\n            best_auc = auc\n            best_feature = feature\n        current_list.remove(feature)\n    return best_feature, best_auc\n</code></pre>"},{"location":"api/#src.aprofs.utils.calculate_all_row_sum","title":"<code>calculate_all_row_sum(data, mean_value, link_model)</code>","text":"<p>Calculates the row sum of all columns in a Shapley values DataFrame and applies a link function to the result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input Shapley values DataFrame.</p> required <code>mean_value</code> <code>float</code> <p>The mean shapley value to be added to the row sum.</p> required <code>link_model</code> <code>aprofs model object</code> <p>An object that allows to calculate the performance of the model.</p> required <p>Returns:</p> Type Description <code>Union[float, Series]</code> <p>Union[float, pd.Series]: The result of applying the link function to the row sum.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n&gt;&gt;&gt; mean_value = 10.0\n&gt;&gt;&gt; link_function = lambda x: x ** 2\n&gt;&gt;&gt; calculate_all_row_sum(data, mean_value, link_function)\n225.0\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def calculate_all_row_sum(data: pd.DataFrame, mean_value: float, link_model: LinkModels) -&gt; Union[float, pd.Series]:\n    \"\"\"\n    Calculates the row sum of **all columns** in a Shapley values DataFrame and applies a link function to the result.\n\n    Args:\n        data (pd.DataFrame): The input Shapley values DataFrame.\n        mean_value (float): The mean shapley value to be added to the row sum.\n        link_model (aprofs model object): An object that allows to calculate the performance of the model.\n\n    Returns:\n        Union[float, pd.Series]: The result of applying the link function to the row sum.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        &gt;&gt;&gt; mean_value = 10.0\n        &gt;&gt;&gt; link_function = lambda x: x ** 2\n        &gt;&gt;&gt; calculate_all_row_sum(data, mean_value, link_function)\n        225.0\n    \"\"\"\n    return link_model.link_calculate(mean_value + data.sum(axis=1))\n</code></pre>"},{"location":"api/#src.aprofs.utils.calculate_row_sum","title":"<code>calculate_row_sum(data, mean_value, columns, link_model)</code>","text":"<p>Calculates the row sum of specified columns in a Shapley values DataFrame and applies a link function to the result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame with shapley values.</p> required <code>mean_value</code> <code>float</code> <p>The mean shapley value to be added to the row sum.</p> required <code>columns</code> <code>List[str]</code> <p>The list of column names to be summed.</p> required <code>link_model</code> <code>aprofs model object</code> <p>An object that allows to calculate the performance of the model.</p> required <p>Returns:</p> Type Description <code>Union[float, Series]</code> <p>Union[float, pd.Series]: The result of applying the link function to the row sum.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n&gt;&gt;&gt; mean_value = 10.0\n&gt;&gt;&gt; columns = ['A', 'B']\n&gt;&gt;&gt; link_function = lambda x: x ** 2\n&gt;&gt;&gt; calculate_row_sum(data, mean_value, columns, link_function)\n225.0\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def calculate_row_sum(\n    data: pd.DataFrame, mean_value: float, columns: List[str], link_model: LinkModels\n) -&gt; Union[float, pd.Series]:\n    \"\"\"\n    Calculates the row sum of specified columns in a Shapley values DataFrame and applies a link function to the result.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with shapley values.\n        mean_value (float): The mean shapley value to be added to the row sum.\n        columns (List[str]): The list of column names to be summed.\n        link_model (aprofs model object): An object that allows to calculate the performance of the model.\n\n    Returns:\n        Union[float, pd.Series]: The result of applying the link function to the row sum.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        &gt;&gt;&gt; mean_value = 10.0\n        &gt;&gt;&gt; columns = ['A', 'B']\n        &gt;&gt;&gt; link_function = lambda x: x ** 2\n        &gt;&gt;&gt; calculate_row_sum(data, mean_value, columns, link_function)\n        225.0\n    \"\"\"\n    return link_model.link_calculate(mean_value + data[columns].sum(axis=1))\n</code></pre>"},{"location":"api/#src.aprofs.utils.generate_all_combinations","title":"<code>generate_all_combinations(features)</code>","text":"<p>Generates all possible combinations of the given features list. This will be used to test all possible combinations fo features to find the best combination.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>A list of features.</p> required <p>Returns:</p> Type Description <code>List[Tuple[str]]</code> <p>List[Tuple[str]]: A list of tuples representing all possible combinations of the features.</p> Source code in <code>src/aprofs/utils.py</code> <pre><code>def generate_all_combinations(features: List[str]) -&gt; List[Tuple[str]]:\n    \"\"\"\n    Generates all possible combinations of the given features list.\n    This will be used to test all possible combinations fo features to find the best combination.\n\n    Args:\n        features (List[str]): A list of features.\n\n    Returns:\n        List[Tuple[str]]: A list of tuples representing all possible combinations of the features.\n    \"\"\"\n    all_combinations: List = []\n    for feature_size in range(1, len(features) + 1):\n        all_combinations.extend(combinations(features, feature_size))\n    return all_combinations\n</code></pre>"},{"location":"api/#src.aprofs.utils.get_shap_values","title":"<code>get_shap_values(data, model, type_model='tree')</code>","text":"<p>Calculates the SHAP values and expected average shap value for a given dataset and model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray or DataFrame</code> <p>The input dataset.</p> required <code>model</code> <code>Callable</code> <p>The trained model object.</p> required <code>type_model</code> <code>str</code> <p>type of model: tree based or other. If \"tree\" then TreeExplainer will be use, otherwise a general explainer from the SHAP package is used. Defaults to 'tree'.</p> <code>'tree'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, float]</code> <p>A tuple containing the SHAP values and the Average shap value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from xgboost import XGBClassifier\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; # Imports SHAP Package\n&gt;&gt;&gt; import shap\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load the iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X, y = iris.data, iris.target\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Split the dataset into train and test sets\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Train a model\n&gt;&gt;&gt; model = XGBClassifier()\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate SHAP values and expected value\n&gt;&gt;&gt; shap_values, expected_value = get_shap_tree_values(X_test, model)\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def get_shap_values(data: pd.DataFrame, model: Callable, type_model=\"tree\") -&gt; Tuple[pd.DataFrame, float]:\n    \"\"\"\n    Calculates the SHAP values and expected average shap value for a given dataset and model.\n\n    Args:\n        data (numpy.ndarray or pandas.DataFrame): The input dataset.\n        model: The trained model object.\n        type_model (str): type of model: tree based or other. If \"tree\" then TreeExplainer will be use, otherwise a general explainer from the SHAP package is used. Defaults to 'tree'.\n\n    Returns:\n        tuple: A tuple containing the SHAP values and the Average shap value.\n\n    Examples:\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from xgboost import XGBClassifier\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; # Imports SHAP Package\n        &gt;&gt;&gt; import shap\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load the iris dataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X, y = iris.data, iris.target\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Split the dataset into train and test sets\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Train a model\n        &gt;&gt;&gt; model = XGBClassifier()\n        &gt;&gt;&gt; model.fit(X_train, y_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate SHAP values and expected value\n        &gt;&gt;&gt; shap_values, expected_value = get_shap_tree_values(X_test, model)\n    \"\"\"\n    if type_model == \"tree\":\n        shap_explainer = TreeExplainer(model)\n        shap_valid = shap_explainer.shap_values(data)\n        shap_expected_value = shap_explainer.expected_value\n    else:\n        shap_explainer = Explainer(model)\n        shap_valid = shap_explainer.shap_values(data)\n        shap_expected_value = shap_explainer.expected_value\n\n    if isinstance(shap_valid, list):\n        shap_valid = np.concatenate(shap_valid, axis=1)\n\n    return shap_valid, shap_expected_value\n</code></pre>"},{"location":"api/#src.aprofs.utils.plot_data","title":"<code>plot_data(temp, main_feature, other_features=None, nbins=20, type_bins='qcut', type_plot='prob')</code>","text":"<p>Plot data based on the provided DataFrame and features.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>DataFrame</code> <p>The DataFrame containing the data.</p> required <code>main_feature</code> <code>str</code> <p>The main feature to plot.</p> required <code>other_features</code> <code>Optional[Union[str, List[str]]]</code> <p>Other features to include in the plot. Defaults to None.</p> <code>None</code> <code>nbins</code> <code>int</code> <p>The number of bins. Defaults to 20.</p> <code>20</code> <code>type_bins</code> <code>str</code> <p>The type of binning. Defaults to \"qcut\".</p> <code>'qcut'</code> <code>type_plot</code> <code>str</code> <p>The type of plot. Defaults to \"prob\".</p> <code>'prob'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; temp = pd.DataFrame(...)\n&gt;&gt;&gt; plot_data(temp, \"main_feature\", other_features=[\"feature_1\", \"feature_2\"], nbins=10, type_bins=\"cut\", type_plot=\"raw\")\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def plot_data(  # pylint: disable=too-many-arguments\n    temp: pd.DataFrame,\n    main_feature: str,\n    other_features: Optional[Union[str, List[str]]] = None,\n    nbins: int = 20,\n    type_bins: str = \"qcut\",\n    type_plot: str = \"prob\",\n) -&gt; None:\n    \"\"\"\n    Plot data based on the provided DataFrame and features.\n\n    Args:\n        temp (pd.DataFrame): The DataFrame containing the data.\n        main_feature (str): The main feature to plot.\n        other_features (Optional[Union[str, List[str]]], optional): Other features to include in the plot. Defaults to None.\n        nbins (int, optional): The number of bins. Defaults to 20.\n        type_bins (str, optional): The type of binning. Defaults to \"qcut\".\n        type_plot (str, optional): The type of plot. Defaults to \"prob\".\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; temp = pd.DataFrame(...)\n        &gt;&gt;&gt; plot_data(temp, \"main_feature\", other_features=[\"feature_1\", \"feature_2\"], nbins=10, type_bins=\"cut\", type_plot=\"raw\")\n    \"\"\"\n    if other_features is None:\n        other_features = []\n    if not isinstance(other_features, list):\n        other_features = [other_features]\n    features = []\n    features.append(main_feature)\n    features.extend(other_features)\n\n    if temp[main_feature].unique().shape[0] &lt; 25:\n        temp[\"bins\"] = temp[main_feature].astype(str)\n    elif type_bins == \"cut\":\n        temp[\"bins\"] = pd.cut(temp[main_feature], bins=nbins)\n    elif type_bins == \"qcut\":\n        temp[\"bins\"] = pd.qcut(temp[main_feature], q=nbins)\n    else:\n        print(\"Invalid type_bins value\")\n\n    # Calculate the means for each bin\n    means = temp.groupby(\"bins\", observed=True)[\"target\"].mean()\n\n    means_shap = {}\n    if type_plot == \"raw\":\n        for feature in features:\n            means_shap[feature] = temp.groupby(\"bins\", observed=True)[f\"{feature}_shap\"].mean()\n        means_shap_others = temp.groupby(\"bins\", observed=True)[\"shap_other\"].mean()\n        means_shap_model = temp.groupby(\"bins\", observed=True)[\"shap_model\"].mean()\n    else:\n        for feature in features:\n            means_shap[feature] = temp.groupby(\"bins\", observed=True)[f\"{feature}_shap_prob\"].mean()\n        means_shap_others = temp.groupby(\"bins\", observed=True)[\"shap_prob_other\"].mean()\n        means_shap_model = temp.groupby(\"bins\", observed=True)[\"shap_prob_model\"].mean()\n\n    # Calculate the counts for each bin\n    counts = temp[\"bins\"].value_counts(normalize=True).sort_index()\n\n    # Create a figure\n    fig = go.Figure()\n\n    # Add bar plot for counts on the primary y-axis\n    fig.add_trace(go.Bar(x=counts.index.astype(str), y=counts, name=\"Data\", yaxis=\"y\", marker_color=\"lightgray\"))\n\n    # Add line plots on the secondary y-axis\n    fig.add_trace(go.Scatter(x=means.index.astype(str), y=means, mode=\"lines\", name=\"Observed\", yaxis=\"y2\"))\n\n    for feature in features:\n        fig.add_trace(\n            go.Scatter(\n                x=means_shap[feature].index.astype(str),\n                y=means_shap[feature],\n                mode=\"lines\",\n                name=f\"{feature} shap Mean\",\n                yaxis=\"y2\",\n            )\n        )\n\n    fig.add_trace(\n        go.Scatter(\n            x=means_shap_others.index.astype(str), y=means_shap_others, mode=\"lines\", name=\"Others shaps\", yaxis=\"y2\"\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=means_shap_model.index.astype(str), y=means_shap_model, mode=\"lines\", name=\"Model shaps\", yaxis=\"y2\"\n        )\n    )\n\n    # Update layout to include a secondary y-axis\n    fig.update_layout(\n        yaxis={\"title\": \"Counts\", \"side\": \"left\", \"tickformat\": \".0%\"},\n        yaxis2={\"title\": \"Avg.\", \"side\": \"right\", \"overlaying\": \"y\"},\n    )\n    fig.update_xaxes(title_text=feature)\n    fig.show()\n</code></pre>"},{"location":"api/#src.aprofs.utils.plot_data_compare","title":"<code>plot_data_compare(temp, feature, nbins=20, type_bins='qcut', type_plot='prob')</code>","text":"<p>Plot data based on the provided DataFrame and feature in a way to compare a specific shap.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>DataFrame</code> <p>The DataFrame containing the data.</p> required <code>feature</code> <code>str</code> <p>The main feature to plot.</p> required <code>nbins</code> <code>int</code> <p>The number of bins. Defaults to 20.</p> <code>20</code> <code>type_bins</code> <code>str</code> <p>The type of binning. Defaults to \"qcut\".</p> <code>'qcut'</code> <code>type_plot</code> <code>str</code> <p>The type of plot. Defaults to \"prob\".</p> <code>'prob'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; temp = pd.DataFrame(...)\n&gt;&gt;&gt; plot_data(temp, \"feature_name\", nbins=10, type_bins=\"cut\", type_plot=\"raw\")\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def plot_data_compare(  # pylint: disable=too-many-arguments\n    temp: pd.DataFrame,\n    feature: str,\n    nbins: int = 20,\n    type_bins: str = \"qcut\",\n    type_plot: str = \"prob\",\n) -&gt; None:\n    \"\"\"\n    Plot data based on the provided DataFrame and feature in a way to compare a specific shap.\n\n    Args:\n        temp (pd.DataFrame): The DataFrame containing the data.\n        feature (str): The main feature to plot.\n        nbins (int, optional): The number of bins. Defaults to 20.\n        type_bins (str, optional): The type of binning. Defaults to \"qcut\".\n        type_plot (str, optional): The type of plot. Defaults to \"prob\".\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; temp = pd.DataFrame(...)\n        &gt;&gt;&gt; plot_data(temp, \"feature_name\", nbins=10, type_bins=\"cut\", type_plot=\"raw\")\n    \"\"\"\n\n    if temp[feature].unique().shape[0] &lt; 25:\n        temp[\"bins\"] = temp[feature].astype(str)\n    elif type_bins == \"cut\":\n        temp[\"bins\"] = pd.cut(temp[feature], bins=nbins)\n    elif type_bins == \"qcut\":\n        temp[\"bins\"] = pd.qcut(temp[feature], q=nbins)\n    else:\n        print(\"Invalid type_bins value\")\n\n    # Calculate the means for each bin\n    means = temp.groupby(\"bins\", observed=True)[\"target\"].mean()\n\n    means_shap = {}\n    if type_plot == \"raw\":\n        means_shap[feature] = temp.groupby(\"bins\", observed=True)[f\"{feature}_shap\"].mean()\n        means_shap[f\"{feature}_shap\"] = temp.groupby(\"bins\", observed=True)[f\"{feature}_shap\"].mean()\n        means_shap[f\"{feature}_shap_compare\"] = temp.groupby(\"bins\", observed=True)[f\"{feature}_shap_compare\"].mean()\n        means_shap_model = temp.groupby(\"bins\", observed=True)[\"shap_model\"].mean()\n    else:\n        means_shap[feature] = temp.groupby(\"bins\", observed=True)[f\"{feature}_shap_prob\"].mean()\n        means_shap[f\"{feature}_compare\"] = temp.groupby(\"bins\", observed=True)[f\"{feature}_shap_prob_compare\"].mean()\n        means_shap_model = temp.groupby(\"bins\", observed=True)[\"shap_prob_model\"].mean()\n\n    # Calculate the counts for each bin\n    counts = temp[\"bins\"].value_counts(normalize=True).sort_index()\n\n    # Create a figure\n    fig = go.Figure()\n\n    # Add bar plot for counts on the primary y-axis\n    fig.add_trace(go.Bar(x=counts.index.astype(str), y=counts, name=\"Data\", yaxis=\"y\", marker_color=\"lightgray\"))\n\n    # Add line plots on the secondary y-axis\n    fig.add_trace(go.Scatter(x=means.index.astype(str), y=means, mode=\"lines\", name=\"Observed\", yaxis=\"y2\"))\n\n    fig.add_trace(\n        go.Scatter(\n            x=means_shap[feature].index.astype(str),\n            y=means_shap[feature],\n            mode=\"lines\",\n            name=f\"{feature} shap Mean\",\n            yaxis=\"y2\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=means_shap[f\"{feature}_compare\"].index.astype(str),\n            y=means_shap[f\"{feature}_compare\"],\n            mode=\"lines\",\n            name=f\"{feature} shap Mean compare\",\n            yaxis=\"y2\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=means_shap_model.index.astype(str), y=means_shap_model, mode=\"lines\", name=\"Model shaps\", yaxis=\"y2\"\n        )\n    )\n\n    # Update layout to include a secondary y-axis\n    fig.update_layout(\n        yaxis={\"title\": \"Counts\", \"side\": \"left\", \"tickformat\": \".0%\"},\n        yaxis2={\"title\": \"Avg.\", \"side\": \"right\", \"overlaying\": \"y\"},\n    )\n    fig.update_xaxes(title_text=feature)\n    fig.show()\n</code></pre>"},{"location":"api/#src.aprofs.utils.plot_data_neutral","title":"<code>plot_data_neutral(data, feature, nbins=20, type_bins='qcut', type_plot='prob')</code>","text":"<p>Plot data based on the provided neutralized DataFrame and features.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the neutralize shap data.</p> required <code>feature</code> <code>str</code> <p>The main feature to plot on the x-axis.</p> required <code>nbins</code> <code>int</code> <p>The number of bins. Defaults to 20.</p> <code>20</code> <code>type_bins</code> <code>str</code> <p>The type of binning. Defaults to \"qcut\".</p> <code>'qcut'</code> <code>type_plot</code> <code>str</code> <p>The type of plot. Defaults to \"prob\".</p> <code>'prob'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; temp = pd.DataFrame(...)\n&gt;&gt;&gt; plot_data_neutral(temp, \"main_feature\", other_features=[\"feature_1\", \"feature_2\"], nbins=10, type_bins=\"cut\", type_plot=\"raw\")\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def plot_data_neutral(  # pylint: disable=too-many-arguments\n    data: pd.DataFrame,\n    feature: str,\n    nbins: int = 20,\n    type_bins: str = \"qcut\",\n    type_plot: str = \"prob\",\n) -&gt; None:\n    \"\"\"\n    Plot data based on the provided neutralized DataFrame and features.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the neutralize shap data.\n        feature (str): The main feature to plot on the x-axis.\n        nbins (int, optional): The number of bins. Defaults to 20.\n        type_bins (str, optional): The type of binning. Defaults to \"qcut\".\n        type_plot (str, optional): The type of plot. Defaults to \"prob\".\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; temp = pd.DataFrame(...)\n        &gt;&gt;&gt; plot_data_neutral(temp, \"main_feature\", other_features=[\"feature_1\", \"feature_2\"], nbins=10, type_bins=\"cut\", type_plot=\"raw\")\n    \"\"\"\n\n    if data[feature].unique().shape[0] &lt; 25:\n        data[\"bins\"] = data[feature].astype(str)\n    elif type_bins == \"cut\":\n        data[\"bins\"] = pd.cut(data[feature], bins=nbins)\n    elif type_bins == \"qcut\":\n        data[\"bins\"] = pd.qcut(data[feature], q=nbins)\n    else:\n        print(\"Invalid type_bins value\")\n\n    # Calculate the means for each bin\n    means = data.groupby(\"bins\", observed=True)[\"target\"].mean()\n\n    if type_plot == \"raw\":\n        means_shap_others = data.groupby(\"bins\", observed=True)[\"shap_other\"].mean()\n        means_shap_model = data.groupby(\"bins\", observed=True)[\"shap_model\"].mean()\n    else:\n        means_shap_others = data.groupby(\"bins\", observed=True)[\"shap_prob_other\"].mean()\n        means_shap_model = data.groupby(\"bins\", observed=True)[\"shap_prob_model\"].mean()\n\n    # Calculate the counts for each bin\n    counts = data[\"bins\"].value_counts(normalize=True).sort_index()\n\n    # Create a figure\n    fig = go.Figure()\n\n    # Add bar plot for counts on the primary y-axis\n    fig.add_trace(go.Bar(x=counts.index.astype(str), y=counts, name=\"Data\", yaxis=\"y\", marker_color=\"lightgray\"))\n\n    # Add line plots on the secondary y-axis\n    fig.add_trace(go.Scatter(x=means.index.astype(str), y=means, mode=\"lines\", name=\"Observed\", yaxis=\"y2\"))\n\n    fig.add_trace(\n        go.Scatter(\n            x=means_shap_others.index.astype(str),\n            y=means_shap_others,\n            mode=\"lines\",\n            name=\"Neutralized shaps\",\n            yaxis=\"y2\",\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=means_shap_model.index.astype(str),\n            y=means_shap_model,\n            mode=\"lines\",\n            name=\"Original Model shaps\",\n            yaxis=\"y2\",\n        )\n    )\n\n    # Update layout to include a secondary y-axis\n    fig.update_layout(\n        yaxis={\"title\": \"Counts\", \"side\": \"left\", \"tickformat\": \".0%\"},\n        yaxis2={\"title\": \"Avg.\", \"side\": \"right\", \"overlaying\": \"y\"},\n    )\n    # Add title to x-axis\n    fig.update_xaxes(title_text=feature)\n\n    fig.show()\n</code></pre>"},{"location":"api/#src.aprofs.utils.random_sort_shaps","title":"<code>random_sort_shaps(shaps_values, shap_expected_value, feature_name, y_target, link_model)</code>","text":"<p>Randomly shuffles the values of a specific feature in the SHAP values DataFrame, calculates the row sum, and returns the ROC AUC score.</p> <p>Parameters:</p> Name Type Description Default <code>shaps_values</code> <code>DataFrame</code> <p>The SHAP values DataFrame.</p> required <code>shap_expected_value</code> <code>float</code> <p>The expected SHAP value.</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature to shuffle.</p> required <code>y_target</code> <code>Union[Series, ndarray]</code> <p>The target variable.</p> required <code>link_model</code> <code>aprofs model object</code> <p>An object that allows to calculate the performance of the model.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The ROC AUC score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.metrics import roc_auc_score\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate synthetic data\n&gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Train a logistic regression model\n&gt;&gt;&gt; model = LogisticRegression()\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate SHAP values and expected value\n&gt;&gt;&gt; shap_values, expected_value = get_shap_tree_values(X_test, model)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate ROC AUC score with shuffled feature\n&gt;&gt;&gt; roc_score = random_sort_shaps(shap_values, expected_value, 'feature_1', y_test, link_function='logistic')\n&gt;&gt;&gt; print(roc_score)\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def random_sort_shaps(\n    shaps_values: pd.DataFrame,\n    shap_expected_value: float,\n    feature_name: str,\n    y_target: Union[pd.Series, np.ndarray],\n    link_model: LinkModels,\n) -&gt; float:\n    \"\"\"\n    Randomly shuffles the values of a specific feature in the SHAP values DataFrame,\n    calculates the row sum, and returns the ROC AUC score.\n\n    Args:\n        shaps_values (pd.DataFrame): The SHAP values DataFrame.\n        shap_expected_value (float): The expected SHAP value.\n        feature_name (str): The name of the feature to shuffle.\n        y_target (Union[pd.Series, np.ndarray]): The target variable.\n        link_model (aprofs model object): An object that allows to calculate the performance of the model.\n\n    Returns:\n        float: The ROC AUC score.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; from sklearn.metrics import roc_auc_score\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate synthetic data\n        &gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Train a logistic regression model\n        &gt;&gt;&gt; model = LogisticRegression()\n        &gt;&gt;&gt; model.fit(X_train, y_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate SHAP values and expected value\n        &gt;&gt;&gt; shap_values, expected_value = get_shap_tree_values(X_test, model)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate ROC AUC score with shuffled feature\n        &gt;&gt;&gt; roc_score = random_sort_shaps(shap_values, expected_value, 'feature_1', y_test, link_function='logistic')\n        &gt;&gt;&gt; print(roc_score)\n    \"\"\"\n    shaps_values_shuffled = shaps_values.sample(frac=1)  # shuffle\n    shaps_values_shuffled.reset_index(inplace=True, drop=True)\n\n    new_shap_table = shaps_values.copy()\n    new_shap_table.reset_index(inplace=True, drop=True)\n\n    new_shap_table[feature_name] = shaps_values_shuffled[feature_name]\n    approx_pred_valid = calculate_all_row_sum(new_shap_table, shap_expected_value, link_model)\n\n    return link_model.performance_fit(y_target, approx_pred_valid)\n</code></pre>"},{"location":"api/#src.aprofs.utils.random_sort_shaps_column","title":"<code>random_sort_shaps_column(shaps_values, shap_mean_value, target_column, feature, link_model, original=False)</code>","text":"<p>Randomly shuffles the values of a specific feature in the SHAP values DataFrame, calculates the row sum, and returns the ROC AUC score.</p> <p>Parameters:</p> Name Type Description Default <code>shaps_values</code> <code>DataFrame</code> <p>The SHAP values DataFrame.</p> required <code>shap_mean_value</code> <code>float</code> <p>The mean SHAP value.</p> required <code>target_column</code> <code>Union[Series, ndarray]</code> <p>The target variable.</p> required <code>feature</code> <code>str</code> <p>The name of the feature to shuffle.</p> required <code>link_model</code> <code>aprofs model object</code> <p>An object that allows to calculate the performance of the model.</p> required <code>original</code> <code>bool</code> <p>Whether to use the original feature values or shuffled values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The ROC AUC score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.metrics import roc_auc_score\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate synthetic data\n&gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Train a logistic regression model\n&gt;&gt;&gt; model = LogisticRegression()\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate SHAP values and expected value\n&gt;&gt;&gt; shap_values, expected_value = get_shap_tree_values(X_test, model)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate ROC AUC score with shuffled feature\n&gt;&gt;&gt; roc_score = random_sort_shaps_column(shap_values, expected_value, y_test, 'feature_1', link_function='logistic')\n&gt;&gt;&gt; print(roc_score)\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def random_sort_shaps_column(  # pylint: disable=too-many-arguments\n    shaps_values: pd.DataFrame,\n    shap_mean_value: float,\n    target_column: Union[pd.Series, np.ndarray],\n    feature: str,\n    link_model: LinkModels,\n    original: bool = False,\n) -&gt; float:\n    \"\"\"\n    Randomly shuffles the values of a specific feature in the SHAP values DataFrame,\n    calculates the row sum, and returns the ROC AUC score.\n\n    Args:\n        shaps_values (pd.DataFrame): The SHAP values DataFrame.\n        shap_mean_value (float): The mean SHAP value.\n        target_column (Union[pd.Series, np.ndarray]): The target variable.\n        feature (str): The name of the feature to shuffle.\n        link_model (aprofs model object): An object that allows to calculate the performance of the model.\n        original (bool, optional): Whether to use the original feature values or shuffled values. Defaults to False.\n\n    Returns:\n        float: The ROC AUC score.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; from sklearn.metrics import roc_auc_score\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate synthetic data\n        &gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Train a logistic regression model\n        &gt;&gt;&gt; model = LogisticRegression()\n        &gt;&gt;&gt; model.fit(X_train, y_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate SHAP values and expected value\n        &gt;&gt;&gt; shap_values, expected_value = get_shap_tree_values(X_test, model)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate ROC AUC score with shuffled feature\n        &gt;&gt;&gt; roc_score = random_sort_shaps_column(shap_values, expected_value, y_test, 'feature_1', link_function='logistic')\n        &gt;&gt;&gt; print(roc_score)\n    \"\"\"\n    shaps_values_original = shaps_values.copy()\n    shaps_values_original.reset_index(inplace=True, drop=True)\n\n    shaps_values_shuffled = shaps_values.sample(frac=1)  # shuffle\n    shaps_values_shuffled.reset_index(inplace=True, drop=True)\n\n    # Calculate the average values of each column\n    average_values = shaps_values.mean()\n    new_shap_table = shaps_values.copy()\n    new_shap_table.reset_index(inplace=True, drop=True)\n    for feature_name in shaps_values.columns:\n        new_shap_table[feature_name] = average_values[feature_name]\n\n    if original:\n        new_shap_table[feature] = shaps_values_original[feature]\n    else:\n        new_shap_table[feature] = shaps_values_shuffled[feature]\n\n    approx_pred_valid = calculate_all_row_sum(new_shap_table, shap_mean_value, link_model)\n\n    return link_model.performance_fit(target_column, approx_pred_valid)\n</code></pre>"},{"location":"api/#src.aprofs.utils.temp_neutral_plot_data","title":"<code>temp_neutral_plot_data(aprofs_obj, features)</code>","text":"<p>Generate a temporary DataFrame for plotting purposes.</p> <p>Parameters:</p> Name Type Description Default <code>aprofs_obj</code> <code>Aprofs Object</code> <p>An instance of the Aprofs class.</p> required <code>features</code> <code>List[str]</code> <p>A list of feature names that will be neutralized. The shapley values for this will be just the average values. This way the break the segmentation of the feature, maintaining the global effect of all the others.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The temporary DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; aprofs_obj = Aprofs Object(...)\n&gt;&gt;&gt; features = ['feature_1', 'feature_2']\n&gt;&gt;&gt; temp = temp_neutral_plot_data(aprofs_obj, features)\n&gt;&gt;&gt; print(temp.head())\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def temp_neutral_plot_data(aprofs_obj, features: List[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a temporary DataFrame for plotting purposes.\n\n    Args:\n        aprofs_obj (Aprofs Object): An instance of the Aprofs class.\n        features (List[str]): A list of feature names that will be neutralized. The shapley values for this will be just the average values. This way the break the segmentation of the feature, maintaining the global effect of all the others.\n\n    Returns:\n        pd.DataFrame: The temporary DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; aprofs_obj = Aprofs Object(...)\n        &gt;&gt;&gt; features = ['feature_1', 'feature_2']\n        &gt;&gt;&gt; temp = temp_neutral_plot_data(aprofs_obj, features)\n        &gt;&gt;&gt; print(temp.head())\n    \"\"\"\n    if not isinstance(features, list):\n        features = [features]\n\n    temp = pd.DataFrame(\n        {\n            \"target\": aprofs_obj.target_column,\n        }\n    )\n\n    for feat in features:\n        temp[feat] = aprofs_obj.current_data[feat].values  # adding features to data\n\n    temp[\"shap_other\"] = (\n        aprofs_obj.shap_mean\n        + aprofs_obj.shap_values[[col for col in aprofs_obj.shap_values.columns if col not in features]].sum(axis=1)\n        + aprofs_obj.shap_values[features]\n        .sum(axis=1)\n        .mean()  # sums the columns of the features and calculate the average value\n    )\n    temp[\"shap_prob_other\"] = 1 / (1 + np.exp(-temp[\"shap_other\"]))\n    temp[\"shap_model\"] = aprofs_obj.shap_mean + aprofs_obj.shap_values.sum(axis=1)\n    temp[\"shap_prob_model\"] = 1 / (1 + np.exp(-temp[\"shap_model\"]))\n\n    return temp\n</code></pre>"},{"location":"api/#src.aprofs.utils.temp_plot_compare_data","title":"<code>temp_plot_compare_data(aprofs_obj_self, aprofs_obj, feature)</code>","text":"<p>Generate a temporary DataFrame for plotting purposes.</p> <p>Parameters:</p> Name Type Description Default <code>aprofs_obj_self</code> <code>Aprofs Object</code> <p>An instance of the Aprofs class.</p> required <code>aprofs_obj</code> <code>Aprofs Object</code> <p>An instance of the Aprofs class.</p> required <code>feature</code> <code>str</code> <p>feature to compare.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The temporary DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; aprofs_obj = Aprofs Object(...)\n&gt;&gt;&gt; aprofs_obj_2_compare = Aprofs Object(...)\n&gt;&gt;&gt; features = 'feature_1'\n&gt;&gt;&gt; temp = temp_plot_data(aprofs_obj,aprofs_obj_2_compare, feature)\n&gt;&gt;&gt; print(temp.head())\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def temp_plot_compare_data(aprofs_obj_self, aprofs_obj, feature: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a temporary DataFrame for plotting purposes.\n\n    Args:\n        aprofs_obj_self (Aprofs Object): An instance of the Aprofs class.\n        aprofs_obj (Aprofs Object): An instance of the Aprofs class.\n        feature (str): feature to compare.\n\n    Returns:\n        pd.DataFrame: The temporary DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; aprofs_obj = Aprofs Object(...)\n        &gt;&gt;&gt; aprofs_obj_2_compare = Aprofs Object(...)\n        &gt;&gt;&gt; features = 'feature_1'\n        &gt;&gt;&gt; temp = temp_plot_data(aprofs_obj,aprofs_obj_2_compare, feature)\n        &gt;&gt;&gt; print(temp.head())\n    \"\"\"\n\n    temp = pd.DataFrame(\n        {\n            \"target\": aprofs_obj_self.target_column,\n        }\n    )\n\n    # self data\n    temp[feature] = aprofs_obj_self.current_data[feature].values\n    temp[f\"{feature}_shap\"] = aprofs_obj_self.shap_mean + aprofs_obj_self.shap_values[feature].values\n    temp[f\"{feature}_shap_prob\"] = 1 / (1 + np.exp(-temp[f\"{feature}_shap\"]))\n\n    # compare data\n    temp[f\"{feature}_shap_compare\"] = aprofs_obj.shap_mean + aprofs_obj.shap_values[feature].values\n    temp[f\"{feature}_shap_prob_compare\"] = 1 / (1 + np.exp(-temp[f\"{feature}_shap_compare\"]))\n\n    # model probabilities data\n    temp[\"shap_model\"] = aprofs_obj.shap_mean + aprofs_obj.shap_values.sum(axis=1)\n    temp[\"shap_prob_model\"] = 1 / (1 + np.exp(-temp[\"shap_model\"]))\n\n    return temp\n</code></pre>"},{"location":"api/#src.aprofs.utils.temp_plot_data","title":"<code>temp_plot_data(aprofs_obj, features)</code>","text":"<p>Generate a temporary DataFrame for plotting purposes.</p> <p>Parameters:</p> Name Type Description Default <code>aprofs_obj</code> <code>Aprofs Object</code> <p>An instance of the Aprofs class.</p> required <code>features</code> <code>List[str]</code> <p>A list of feature names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The temporary DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; aprofs_obj = Aprofs Object(...)\n&gt;&gt;&gt; features = ['feature_1', 'feature_2']\n&gt;&gt;&gt; temp = temp_plot_data(aprofs_obj, features)\n&gt;&gt;&gt; print(temp.head())\n</code></pre> Source code in <code>src/aprofs/utils.py</code> <pre><code>def temp_plot_data(aprofs_obj, features: List[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a temporary DataFrame for plotting purposes.\n\n    Args:\n        aprofs_obj (Aprofs Object): An instance of the Aprofs class.\n        features (List[str]): A list of feature names.\n\n    Returns:\n        pd.DataFrame: The temporary DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; aprofs_obj = Aprofs Object(...)\n        &gt;&gt;&gt; features = ['feature_1', 'feature_2']\n        &gt;&gt;&gt; temp = temp_plot_data(aprofs_obj, features)\n        &gt;&gt;&gt; print(temp.head())\n    \"\"\"\n    if not isinstance(features, list):\n        features = [features]\n\n    temp = pd.DataFrame(\n        {\n            \"target\": aprofs_obj.target_column,\n        }\n    )\n\n    for feature in features:\n        temp[feature] = aprofs_obj.current_data[feature].values\n        temp[f\"{feature}_shap\"] = aprofs_obj.shap_mean + aprofs_obj.shap_values[feature].values\n        temp[f\"{feature}_shap_prob\"] = 1 / (1 + np.exp(-temp[f\"{feature}_shap\"]))\n\n    temp[\"shap_other\"] = aprofs_obj.shap_mean + aprofs_obj.shap_values[\n        [col for col in aprofs_obj.shap_values.columns if col not in features]\n    ].sum(axis=1)\n    temp[\"shap_prob_other\"] = 1 / (1 + np.exp(-temp[\"shap_other\"]))\n    temp[\"shap_model\"] = aprofs_obj.shap_mean + aprofs_obj.shap_values.sum(axis=1)\n    temp[\"shap_prob_model\"] = 1 / (1 + np.exp(-temp[\"shap_model\"]))\n\n    return temp\n</code></pre>"},{"location":"guide/","title":"This part of the project documentation focuses on explaining how the code should be used.","text":""},{"location":"guide/#get-you-first-aprofs-object-up","title":"Get you first aprofs object up","text":"<p>The initial idea of the project was to create a structure that contains all the information needed to use the shapley values along side with you calibration data to get the results you need.</p> <p>Once instantiated this object with all the proper inputs, we just need to use the correct methods to get our results.</p> <p>It's very important to get the shapley values calculated before using the aprofs object. I've added some wrapper method called calculate_shaps that will help you you bring your own model.</p> <p>To get the shapley table you just used some of SHAP package functionality like tje one you can see below:</p> TreeExplainer<pre><code>    shap_explainer = TreeExplainer(model) # Shap explainer for tree models\n    shap_valid = shap_explainer.shap_values(data) # get you sdapley values for your data\n    shap_expected_value = shap_explainer.expected_value # averaga shapley value\n</code></pre> <p>Please reach to the API to get all the details of how to create the object and use the method. Basically you need the calibration data, the target column for that data and an ML model that can be scored on the calibration data. A structural detail is that the model will not be saved inside the aprofs object. I didn't want to bloat the object more than needed.</p> <pre><code>from aprofs import code\nfrom apros.models import ClassificationLogisticLink\n\n# Instantiate the type of model/link that you used for training\nlogit_model = ClassificationLogisticLink()\n\n\naprofs_obj = code.Aprofs(X_calibration, y_target_calibration, link_model=logit_model)\n</code></pre> <p>In this example the link logit model its used by AProfs internally to be able to calculate the prediction using the shapley values and the same link function that you model is using. The details behind the scenes is the following: the sum of the shapley values are teh raw values obtained before applying the link function of your model, in the case os classification usually its the odds ration. Please investigate your model to know what is the link function used if any.</p> <p>you can extend AProfs to a any link model, you just nee to extend the model class like this:</p> <pre><code>from aprofs import code\nfrom apros.models import LinkModels\n\nclass MyModelLink(LinkModels):\n    \"\"\"This class implements the interface for classification with logistic link\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(type_model=\"yuur_type_of_model\", type_link=\"typs pf link\", perform=\"maximize\")\n        # the type ahd type line are informative, the perform/performance need to be \"maximize\" or \"minimize\"\n\n    def performance_fit(self, target: Union[np.ndarray, pd.Series], prediction: Union[np.ndarray, pd.Series]) -&gt; float:\n        return None #you models performance function, need to use (target, prediction)\n\n    def link_calculate(\n        self, inv_prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        if not isinstance(inv_prediction, (int, float, np.ndarray, pd.Series)):\n            raise ValueError(\"Invalid input type for link_calculate\")\n        return None #you link function goes here\n\n    def inv_link_calculate(\n        self, prediction: Union[int, float, np.ndarray, pd.Series]\n    ) -&gt; Union[int, float, np.ndarray, pd.Series]:\n        return None\u00df #you inverse link function goes here\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}() with type model {self.type_model} and type link {self.type_link}\"\n\n\n# Instantiate the type of model/link that you used for training\nlogit_model = ClassificationLogisticLink()\n\n\naprofs_obj = code.Aprofs(X_calibration, y_target_calibration, link_model=logit_model)\n</code></pre> <p>[!WARNING] At the moment only the logistic or binary models where tested and develops, more to come in the future and also Only AUC as performance metric is available</p> <p>You can add the shapley values table and mean shapley value with pre-calculated results like this:</p> <p><pre><code>aprofs_obj.shap_values = pre_calc_shaps_df\naprofs_obj.shap_mean = pre_calc_shaps_mean\n</code></pre> just simple attribution.</p> <p>A note here, the aprofs_obj shap values values at this moment need to like a pandas (pd.df) dataframe. To do this use this snippet of code below for example:</p> Shap to dataframe<pre><code>pre_calc_shaps_df= pd.DataFrame(shap_values, index=self.X_calibration.index, columns=self.X_calibration.columns)\n</code></pre> <p>If you use the wrapper that is provided, this will work:</p> Shap to dataframe<pre><code>aprofs_obj.calculate_shaps(model)\n</code></pre>"},{"location":"guide/#select-features-with-aprofs-object","title":"Select features with aprofs object","text":"<p>After having your shapley value created or added into the aprofs object we can start using the built-in functionality.</p> <p>The first step will be using the feature selection functionality. At the moment I have implemented the brute force method and the greedy forward selection method. More will be added in the future.</p> <p>The idea of brute force is that this method will look into all possibilities of the feature combinations and calculate the model performance using the approximate predictions possible with the shapley values table.</p> <p>The other approach, greedy forward its very intuitive, lets see:     0 - Initialize the winning solution with the best individual feature.     1 - Select best feature individually not in the winning solution.     2 - Add this feature into the best solution and test if it improves the performance.     3 - If yes in the previous step: keep it in the winning solution and start from 1, if no we drop the feature from the winning solution and back to 1</p> <p>As an example, the performance of the feature A and feature B. will be calculated using the following logic:</p> <pre><code>- (Shapley A + Shapley Value B + Shap mean value)\n</code></pre> <p>To this score then its applied the inverse of the link function the get the final prediction. In the case of binary classification function applied the sigmoid function into the model score.</p> <p>To run feature selection use the following method.</p> Feature selection<pre><code>aprofs_obj.brute_force_selection([\"list of features\"])\n</code></pre> <p>It will return a list with the best subset of features.</p> <p>For the greedy gready_forward_selection use:</p> Feature selection<pre><code>aprofs_obj.brute_force_selection([\"list of features\"])\n</code></pre>"},{"location":"guide/#visualize-your-shapley-value","title":"Visualize your shapley value","text":"<p>To validate and understand our model its interesting to look into the predicted vs observed values comparing their behavior along the features used for modelling, but also look at any other feature could look interesting.</p> <p>An interesting add on can be also to add the marginal effect of the shapley values for that feature, this was you can see the overall behavior of the models, but also the marginal effect of the feature on your model building even more intuition and understanding into your package.</p> Feature Visualization<pre><code>aprofs_obj.visualize_feature(\n        main_feature = \"Feature A\",\n        other_features = [\"Feature B\"],\n        nbins = = 20,\n        type_bin = \"qcut\",\n        type_plot = \"prob\")\n</code></pre>"},{"location":"guide/#example-of-visualization","title":"Example of Visualization","text":"<p>We can see the observed target, the model prediction, the average shapley values of the charges feature and the average effect of the other features excluding the charges feature, this way we can see the residual effect of the other features. This is a plotly plot so you can remove the lines that you don't need on the plot. </p>"},{"location":"guide/#compare-your-shapley-value-from-different-models","title":"Compare your shapley value from different models","text":"<p>You create a model model and want to compare the behavior of the same features on different models. Easy, just create two aprofs objects and then used the compare_feature method to visualize the marginal behavior of the feature comparing both models.Please use the same calibration data.</p> Compare Shapley Values<pre><code>aprofs_obj.compare_feature(another_aprofs_obj\n        feature = \"Feature A\",\n        nbins = = 20,\n        type_bin = \"qcut\",\n        type_plot = \"prob\")\n</code></pre>"},{"location":"guide/#example-of-comparign-models-shaps","title":"Example of Comparign models shaps","text":"<p>In this case we have two models and we want to see how the charges features behave giving the differences between the models.</p> <p></p>"},{"location":"guide/#observe-the-behavior-of-the-models-neutralizing-features","title":"Observe the behavior of the models neutralizing features","text":"<p>Another interesting use case could be that you have control features on you models that later on will be removed or getting fixed value for deployment. How to see what happens if you \"neutralize this feature? Just the visualize_neutralized_feature** method.</p> Visualize Neutralized Feature<pre><code>aprofs_obj.visualize_neutralized_feature(\n        main_feature = \"Feature A\",\n        neutralize_features = = \"Feature B\",\n        nbins = = 20,\n        type_bin = \"qcut\",\n        type_plot = \"prob\")\n</code></pre> <p>This will create a plot having the the x-axis the main_feature and will be neutralizing the features defined on the neutralize_features.</p> <p>What is neutralized? Basically the shapley values for the neutralize_features will be replaced bu the average values, this way it neutralized the e effect of differentiation ability of these features.</p>"},{"location":"guide/#example-of-neutralizing-features","title":"Example of neutralizing features","text":"<p>In this example qe have neutralize the children feature, and now we want to see wo the models with this neutralized feature compares with the original model. </p>"},{"location":"reference/","title":"References for this project","text":"<p>The initial idea for this project came from the medium post \u201cApproximate-Predictions\u201d Make Feature Selection Radically Faster from from https://medium.com/@mazzanti.sam..</p> <p>This lead me into new ideas of potential usage (even abuse) of shapley values for machine learning models.</p> <p>In this project I tried to implement the ideas left by Samuelle in the above post, and tried to make it easy an accessible to be used on a MLproject. Also, I tried to increase my DS skills creating a python package and also in some way to give-back to the open-source community what I have been receiving from them over the years.</p> <p>The main premise about the user of this package is that you know how to create you ML models and do some cleaning of the data previously. Please don't throw garbage into any automatic process and expect that roses will come at the end of it.</p> <p>The second big premise is that you are acquainted with shapley values and/or getting a shapley values table from some calibration data and respective predictive model.</p> <p>I will add more resources (if I can) here to help you out:</p>"},{"location":"reference/#shapley-values","title":"Shapley values","text":"<ul> <li> <p>SHAP package used for calculate shapley values shap package</p> </li> <li> <p>Details about the Shapley Tree Explainer shap tree explainer</p> </li> <li> <p>Shapley value reference book Interpretable Machine Learning from Christoph Molnar (https://christophmolnar.com)</p> </li> <li> <p>This interesting book about Shapley values Interpreting Machine Learning Models With SHAP also from Christoph Molnar</p> </li> </ul>"},{"location":"reference/#shap-visualizations","title":"Shap Visualizations","text":"<ul> <li>stackoverflow</li> <li>shap_dependence_plot</li> <li>shap_pdp</li> </ul>"},{"location":"reference/#p-values","title":"P-values","text":"<p>About creating p-values, please take a look at this chapter from the book Feature Engineering and Selection: A Practical Approach for Predictive Models from  Max Kuhn and Kjell Johnson</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This part of the project documentation focuses on a learning-oriented approach. You'll learn how to get started with the code in this project.</p> <p>Note: Expand this section by considering the following points:</p> <ul> <li>Help newcomers with getting started</li> <li>Teach readers about your library by making them     write code</li> <li>Inspire confidence through examples that work for     everyone, repeatably</li> <li>Give readers an immediate sense of achievement</li> <li>Show concrete examples, no abstractions</li> <li>Provide the minimum necessary explanation</li> <li>Avoid any distractions</li> </ul>"}]}